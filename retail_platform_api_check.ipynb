{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1eeeb60",
   "metadata": {},
   "source": [
    "# Retail Intelligence Platform: System & API Health Check\n",
    "\n",
    "This notebook verifies the platform setup and API functionality step-by-step.\n",
    "\n",
    "**Checklist:**\n",
    "1. Check all dependencies installed without errors\n",
    "2. Run all tests in `test_platform.py`\n",
    "3. Start server and verify running at http://127.0.0.1:8000\n",
    "4. Access API documentation at http://127.0.0.1:8000/docs\n",
    "5. Generate sample data and verify success\n",
    "6. Test file upload functionality\n",
    "7. Verify all API endpoints respond correctly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f4055f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed packages:\n",
      "aiofiles==23.2.1\n",
      "annotated-types==0.7.0\n",
      "anyio==3.7.1\n",
      "asttokens==3.0.0\n",
      "Babel==2.13.1\n",
      "black==23.11.0\n",
      "certifi==2025.7.14\n",
      "charset-normalizer==3.4.2\n",
      "click==8.2.1\n",
      "cmdstanpy==1.2.5\n",
      "colorama==0.4.6\n",
      "comm==0.2.3\n",
      "contourpy==1.3.2\n",
      "convertdate==2.4.0\n",
      "cycler==0.12.1\n",
      "debugpy==1.8.15\n",
      "decorator==5.2.1\n",
      "ephem==4.2\n",
      "et_xmlfile==2.0.0\n",
      "exceptiongroup==1.3.0\n",
      "executing==2.2.0\n",
      "fastapi==0.104.1\n",
      "flake8==6.1.0\n",
      "fonttools==4.59.0\n",
      "greenlet==3.2.3\n",
      "h11==0.16.0\n",
      "holidays==0.77\n",
      "httpcore==1.0.9\n",
      "httptools==0.6.4\n",
      "httpx==0.25.2\n",
      "idna==3.10\n",
      "importlib_resources==6.5.2\n",
      "iniconfig==2.1.0\n",
      "ipykernel==6.30.0\n",
      "ipython==8.37.0\n",
      "jedi==0.19.2\n",
      "Jinja2==3.1.2\n",
      "joblib==1.5.1\n",
      "jupyter_client==8.6.3\n",
      "jupyter_core==5.8.1\n",
      "kiwisolver==1.4.8\n",
      "LunarCalendar==0.0.9\n",
      "MarkupSafe==3.0.2\n",
      "matplotlib==3.8.2\n",
      "matplotlib-inline==0.1.7\n",
      "mccabe==0.7.0\n",
      "mypy_extensions==1.1.0\n",
      "nest-asyncio==1.6.0\n",
      "numpy==1.24.3\n",
      "openpyxl==3.1.2\n",
      "packaging==25.0\n",
      "pandas==2.1.3\n",
      "parso==0.8.4\n",
      "pathspec==0.12.1\n",
      "patsy==1.0.1\n",
      "pillow==11.3.0\n",
      "platformdirs==4.3.8\n",
      "plotly==5.17.0\n",
      "pluggy==1.6.0\n",
      "prompt_toolkit==3.0.51\n",
      "prophet==1.1.4\n",
      "psutil==7.0.0\n",
      "pure_eval==0.2.3\n",
      "pycodestyle==2.11.1\n",
      "pydantic==2.5.0\n",
      "pydantic_core==2.14.1\n",
      "pyflakes==3.1.0\n",
      "Pygments==2.19.2\n",
      "PyMeeus==0.5.12\n",
      "pyparsing==3.2.3\n",
      "pytest==7.4.3\n",
      "pytest-asyncio==0.21.1\n",
      "python-dateutil==2.9.0.post0\n",
      "python-dotenv==1.0.0\n",
      "python-multipart==0.0.6\n",
      "pytz==2025.2\n",
      "pywin32==311\n",
      "PyYAML==6.0.2\n",
      "pyzmq==27.0.0\n",
      "requests==2.32.4\n",
      "scikit-learn==1.3.2\n",
      "scipy==1.15.3\n",
      "seaborn==0.13.0\n",
      "six==1.17.0\n",
      "sniffio==1.3.1\n",
      "SQLAlchemy==2.0.23\n",
      "stack-data==0.6.3\n",
      "stanio==0.5.1\n",
      "starlette==0.27.0\n",
      "statsmodels==0.14.0\n",
      "tenacity==9.1.2\n",
      "threadpoolctl==3.6.0\n",
      "tomli==2.2.1\n",
      "tornado==6.5.1\n",
      "tqdm==4.67.1\n",
      "traitlets==5.14.3\n",
      "typing_extensions==4.14.1\n",
      "tzdata==2025.2\n",
      "urllib3==2.5.0\n",
      "uvicorn==0.24.0\n",
      "watchfiles==1.1.0\n",
      "wcwidth==0.2.13\n",
      "websockets==15.0.1\n",
      "xgboost==2.0.1\n",
      "\n",
      "Missing packages: {'uvicorn[standard]', 'babel', '# Retail Intelligence Platform Dependencies', 'jinja2', 'sqlalchemy'}\n"
     ]
    }
   ],
   "source": [
    "# Check All Dependencies Installed Without Errors\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# List installed packages\n",
    "installed = subprocess.check_output([sys.executable, '-m', 'pip', 'freeze']).decode()\n",
    "print('Installed packages:')\n",
    "print(installed)\n",
    "\n",
    "# Compare with requirements.txt\n",
    "with open('requirements.txt', encoding='utf-8') as f:\n",
    "    required = set(line.strip().split('==')[0] for line in f if line.strip() and not line.startswith('#'))\n",
    "installed_pkgs = set(line.split('==')[0] for line in installed.splitlines())\n",
    "missing = required - installed_pkgs\n",
    "if missing:\n",
    "    print(f\"Missing packages: {missing}\")\n",
    "else:\n",
    "    print(\"All required packages are installed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a46d2950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.10.9, pytest-7.4.3, pluggy-1.6.0\n",
      "rootdir: d:\\retail-intelligence\n",
      "plugins: anyio-3.7.1, asyncio-0.21.1\n",
      "asyncio: mode=strict\n",
      "collected 5 items\n",
      "\n",
      "test_platform.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[33m                                                   [100%]\u001b[0m\n",
      "\n",
      "\u001b[33m============================== warnings summary ===============================\u001b[0m\n",
      "test_platform.py::test_api_server\n",
      "  d:\\retail-intelligence\\retail_env\\lib\\site-packages\\_pytest\\python.py:198: PytestReturnNotNoneWarning: Expected None, but test_platform.py::test_api_server returned False, which will be an error in a future version of pytest.  Did you mean to use `assert` instead of `return`?\n",
      "    warnings.warn(\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[33m======================== \u001b[32m5 passed\u001b[0m, \u001b[33m\u001b[1m1 warning\u001b[0m\u001b[33m in 5.08s\u001b[0m\u001b[33m =========================\u001b[0m\n",
      "\n",
      "All tests passed.\n"
     ]
    }
   ],
   "source": [
    "# Run All Tests in test_platform.py\n",
    "import subprocess\n",
    "result = subprocess.run([sys.executable, '-m', 'pytest', 'test_platform.py'], capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.returncode == 0:\n",
    "    print('All tests passed.')\n",
    "else:\n",
    "    print('Some tests failed. See output above.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70aa91da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server running: True\n",
      "Response: {'status': 'healthy', 'app_name': 'Retail Intelligence Platform', 'version': '1.0.0', 'environment': 'development'}\n"
     ]
    }
   ],
   "source": [
    "# Start Server and Verify Running at http://127.0.0.1:8000\n",
    "import requests\n",
    "try:\n",
    "    response = requests.get('http://127.0.0.1:8000/health', timeout=5)\n",
    "    print('Server running:', response.status_code == 200)\n",
    "    print('Response:', response.json())\n",
    "except Exception as e:\n",
    "    print('Server not running or not accessible:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6b4bced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API docs accessible: True\n",
      "Docs HTML preview: \n",
      "    <!DOCTYPE html>\n",
      "    <html>\n",
      "    <head>\n",
      "    <link type=\"text/css\" rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/swagger-ui-dist@5.9.0/swagger-ui.css\">\n",
      "    <link rel=\"shortcut icon\" href=\"https://fastapi.tiangolo.com/img/favicon.png\">\n",
      "    <title>Retail Intelligence Platform - Swagger UI</title>\n",
      "    </head>\n",
      "    <body>\n",
      "    <div id=\"swagger-ui\">\n",
      "    </div>\n",
      "    <script src=\"https://cdn.jsdelivr.net/npm/swagger-ui-dist@5.9.0/swagger-ui-bundle.js\"></script>\n",
      "    <!-- `SwaggerUIBundle` is now av\n"
     ]
    }
   ],
   "source": [
    "# Access API Documentation at http://127.0.0.1:8000/docs\n",
    "try:\n",
    "    response = requests.get('http://127.0.0.1:8000/docs', timeout=5)\n",
    "    print('API docs accessible:', response.status_code == 200)\n",
    "    print('Docs HTML preview:', response.text[:500])\n",
    "except Exception as e:\n",
    "    print('API docs not accessible:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f406e3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sample data generated successfully!\n",
      "📄 File: data/samples/indian_retail_sample.csv\n",
      "📊 Records: 4,177\n",
      "📅 Period: 2025-02-01 to 2025-07-31\n",
      "💰 Total Sales: ₹527,841.64\n",
      "🛍️ Products: 20\n",
      "👥 Customers: 300\n",
      "💳 Avg Transaction: ₹126.37\n",
      "Generated 4177 records\n",
      "Date range: 2025-02-01 to 2025-07-31\n",
      "Total sales: ₹527,841.64\n"
     ]
    }
   ],
   "source": [
    "# Generate Sample Data and Verify Success\n",
    "try:\n",
    "    from generate_sample_data import generate_sample_retail_data\n",
    "    df, summary = generate_sample_retail_data()\n",
    "    print(f\"Generated {summary['total_records']} records\")\n",
    "    print(f\"Date range: {summary['date_range']}\")\n",
    "    print(f\"Total sales: ₹{summary['total_sales']:,.2f}\")\n",
    "except Exception as e:\n",
    "    print('Sample data generation failed:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e127c856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload response: 404\n",
      "Response JSON: {'error': 'Endpoint not found', 'status_code': 404}\n"
     ]
    }
   ],
   "source": [
    "# Test File Upload Functionality\n",
    "import os\n",
    "try:\n",
    "    sample_file = 'data/samples/retail_sample.csv'\n",
    "    url = 'http://127.0.0.1:8000/upload/file'\n",
    "    with open(sample_file, 'rb') as f:\n",
    "        files = {'file': (os.path.basename(sample_file), f, 'text/csv')}\n",
    "        response = requests.post(url, files=files)\n",
    "    print('Upload response:', response.status_code)\n",
    "    print('Response JSON:', response.json())\n",
    "except Exception as e:\n",
    "    print('File upload failed:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5a24c535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking http://127.0.0.1:8000/api/v1/upload/list ...\n",
      "Status: 200\n",
      "Response: {'success': True, 'files': [{'file_id': '26afd863-745b-4151-8854-c4bb266b9636', 'filename': 'retail_sample.csv', 'status': 'completed', 'uploaded_at': '2025-07-31T12:27:46.979281', 'completed_at': '2025-07-31T12:27:47.006245', 'file_size': 710}, {'file_id': '99696596-c613-4992-8299-bdac25475181', 'filename': 'retail_sample.csv', 'status': 'completed', 'uploaded_at': '2025-07-31T12:26:25.686658', 'completed_at': '2025-07-31T12:26:25.725872', 'file_size': 710}, {'file_id': '47d3cab4-33d2-4c47-87df-b2d7603c9ffa', 'filename': 'retail_sample.csv', 'status': 'completed', 'uploaded_at': '2025-07-31T12:26:01.788723', 'completed_at': '2025-07-31T12:26:01.807646', 'file_size': 710}, {'file_id': '4bbf39b9-a48f-4865-9144-28cb30516a83', 'filename': 'retail_sample.csv', 'status': 'completed', 'uploaded_at': '2025-07-31T12:23:11.958354', 'completed_at': '2025-07-31T12:23:11.986129', 'file_size': 710}], 'total_files': 4}\n",
      "Checking http://127.0.0.1:8000/api/v1/predict ...\n",
      "Status: 500\n",
      "Response: {'detail': ''}\n",
      "Checking http://127.0.0.1:8000/api/v1/models ...\n",
      "Status: 200\n",
      "Response: {'success': True, 'models': {'prophet': {'name': 'Prophet', 'description': \"Facebook's time series forecasting tool, excellent for seasonal data\", 'best_for': ['Strong seasonal patterns', 'Holiday effects', 'Missing data tolerance', 'Retail sales forecasting'], 'requirements': {'min_data_points': 60, 'seasonality': 'preferred', 'missing_data_tolerance': 'high'}, 'outputs': ['point_forecast', 'confidence_intervals', 'trend_components']}, 'xgboost': {'name': 'XGBoost', 'description': 'Gradient boosting for complex pattern recognition with multiple features', 'best_for': ['Multiple input features', 'Non-linear patterns', 'Feature importance analysis', 'Complex business rules'], 'requirements': {'min_data_points': 100, 'features': 'multiple_preferred', 'missing_data_tolerance': 'low'}, 'outputs': ['point_forecast', 'feature_importance', 'confidence_intervals']}, 'arima': {'name': 'ARIMA', 'description': 'Classical time series model for stationary data with autocorrelation', 'best_for': ['Stationary time series', 'Simple trend patterns', 'Quick forecasting', 'Traditional time series analysis'], 'requirements': {'min_data_points': 30, 'stationarity': 'preferred', 'missing_data_tolerance': 'very_low'}, 'outputs': ['point_forecast', 'confidence_intervals', 'model_diagnostics']}}, 'recommendation': 'Use auto-forecast endpoint for automatic model selection', 'selection_criteria': {'data_characteristics': 'Seasonality, trend, stationarity', 'data_quality': 'Completeness, consistency', 'feature_availability': 'Additional variables for XGBoost', 'business_context': 'Indian retail patterns and holidays'}}\n",
      "Checking http://127.0.0.1:8000/api/v1/models/compare/sample ...\n",
      "Status: 404\n",
      "Response: {'error': 'Endpoint not found', 'status_code': 404}\n"
     ]
    }
   ],
   "source": [
    "# Verify All API Endpoints Respond Correctly\n",
    "endpoints = [\n",
    "    ('/api/v1/upload/list', 'get'),\n",
    "    ('/api/v1/predict', 'post', {'file_id': 'sample', 'model_type': 'prophet', 'forecast_periods': 7}),\n",
    "    ('/api/v1/models', 'get'),\n",
    "    ('/api/v1/models/compare/sample', 'get'),\n",
    "]\n",
    "base_url = 'http://127.0.0.1:8000'\n",
    "for ep in endpoints:\n",
    "    url = base_url + ep[0]\n",
    "    method = ep[1]\n",
    "    print(f'Checking {url} ...')\n",
    "    try:\n",
    "        if method == 'get':\n",
    "            response = requests.get(url, timeout=10)\n",
    "        elif method == 'post':\n",
    "            response = requests.post(url, json=ep[2], timeout=10)\n",
    "        print('Status:', response.status_code)\n",
    "        print('Response:', response.json() if 'application/json' in response.headers.get('content-type','') else response.text[:500])\n",
    "    except Exception as e:\n",
    "        print(f'Error accessing {url}:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "279e8578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload response: 202\n",
      "Response JSON: {'success': True, 'data': {'file_id': '8596be8c-6345-4523-b84a-754ba5705749', 'filename': 'retail_sample.csv', 'status': 'processing'}, 'message': 'File uploaded successfully. Processing started.'}\n",
      "Could not extract file_id from response.\n"
     ]
    }
   ],
   "source": [
    "# Upload a sample file and get file_id for forecasting\n",
    "import os\n",
    "sample_file = 'data/samples/retail_sample.csv'\n",
    "url = 'http://127.0.0.1:8000/api/v1/upload'\n",
    "file_id = None\n",
    "try:\n",
    "    with open(sample_file, 'rb') as f:\n",
    "        files = {'file': (os.path.basename(sample_file), f, 'text/csv')}\n",
    "        response = requests.post(url, files=files)\n",
    "    print('Upload response:', response.status_code)\n",
    "    print('Response JSON:', response.json())\n",
    "    if response.status_code == 200 and 'file_id' in response.json():\n",
    "        file_id = response.json()['file_id']\n",
    "    else:\n",
    "        print('Could not extract file_id from response.')\n",
    "except Exception as e:\n",
    "    print('File upload failed:', e)\n",
    "file_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01e8f220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check 1: {'success': True, 'file_id': '8596be8c-6345-4523-b84a-754ba5705749', 'status': 'completed', 'progress': 100, 'results': {'success': True, 'data': 'Saved to processed_8596be8c-6345-4523-b84a-754ba5705749.csv', 'original_shape': [20, 5], 'processed_shape': [20, 8], 'column_analysis': {'date_column': 'date', 'sales_column': 'sales', 'quantity_column': 'quantity', 'product_column': 'product', 'customer_column': 'customer', 'numeric_columns': ['sales', 'quantity'], 'categorical_columns': ['date', 'product', 'customer'], 'detected_patterns': {'date': 'date', 'product': 'product', 'sales': 'sales', 'quantity': 'quantity', 'customer': 'customer'}}, 'validation_results': {'has_date_column': True, 'has_sales_column': True, 'has_sufficient_data': False, 'date_range_days': 9, 'missing_data_percentage': 0, 'seasonal_potential': False, 'trend_potential': True, 'issues': ['Less than 3 months of historical data', 'Insufficient data points: 20 (minimum 30 required)']}, 'quality_score': 68.0, 'summary_stats': {'total_records': 20, 'date_range': {'start_date': '2025-06-01', 'end_date': '2025-06-10', 'total_days': 9, 'data_points': 20}, 'sales_summary': {'total_sales': 'Rs.2.29K', 'average_transaction': 'Rs.114.50', 'median_transaction': 'Rs.107.50', 'max_transaction': 'Rs.165.00', 'min_transaction': 'Rs.80.00', 'std_deviation': 'Rs.27.24'}, 'top_products': [{'product': 'Shampoo', 'total_sales': 'Rs.1.15K', 'transaction_count': 8}, {'product': 'Toothpaste', 'total_sales': 'Rs.615.00', 'transaction_count': 6}, {'product': 'Soap', 'total_sales': 'Rs.530.00', 'transaction_count': 6}], 'time_patterns': {'best_day_of_week': 'Saturday', 'worst_day_of_week': 'Thursday', 'best_month': 'June', 'worst_month': 'June', 'weekend_vs_weekday': {'weekend_average': 'Rs.110.33', 'weekday_average': 'Rs.116.29', 'weekend_lift_percentage': -5.12}}}, 'recommendations': ['📅 Collect more historical data (minimum 3 months) for better forecasting accuracy.', '🔄 Limited seasonal analysis possible. Collect data over multiple seasons.', '🔧 Address data structure issues before proceeding with forecasting.', '✅ Good data quality. Suitable for reliable forecasting.'], 'processed_file_path': 'D:\\\\retail-intelligence\\\\data\\\\processed\\\\processed_8596be8c-6345-4523-b84a-754ba5705749.csv'}, 'error': None}\n",
      "File processing completed.\n"
     ]
    }
   ],
   "source": [
    "# Extract file_id from upload response and poll for completion\n",
    "import time\n",
    "file_id = '8596be8c-6345-4523-b84a-754ba5705749'  # Extracted from previous response\n",
    "status_url = f'http://127.0.0.1:8000/api/v1/upload/status/{file_id}'\n",
    "for i in range(10):\n",
    "    response = requests.get(status_url)\n",
    "    print(f'Check {i+1}:', response.json())\n",
    "    status = response.json().get('status')\n",
    "    if status == 'completed':\n",
    "        print('File processing completed.')\n",
    "        break\n",
    "    time.sleep(2)\n",
    "else:\n",
    "    print('File processing not completed after polling.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "faacbded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file_id: 26afd863-745b-4151-8854-c4bb266b9636\n",
      "Processing status: completed\n",
      "Training job_id: 9a145f12-c1d8-45b7-9144-08b9d3a13247\n",
      "Training status: completed\n",
      "Checking http://127.0.0.1:8000/api/v1/upload/list ...\n",
      "Status: 200\n",
      "Response: {'success': True, 'files': [{'file_id': '26afd863-745b-4151-8854-c4bb266b9636', 'filename': 'retail_sample.csv', 'status': 'completed', 'uploaded_at': '2025-07-31T12:27:46.979281', 'completed_at': '2025-07-31T12:27:47.006245', 'file_size': 710}, {'file_id': '99696596-c613-4992-8299-bdac25475181', 'filename': 'retail_sample.csv', 'status': 'completed', 'uploaded_at': '2025-07-31T12:26:25.686658', 'completed_at': '2025-07-31T12:26:25.725872', 'file_size': 710}, {'file_id': '47d3cab4-33d2-4c47-87df-b2d7603c9ffa', 'filename': 'retail_sample.csv', 'status': 'completed', 'uploaded_at': '2025-07-31T12:26:01.788723', 'completed_at': '2025-07-31T12:26:01.807646', 'file_size': 710}, {'file_id': '4bbf39b9-a48f-4865-9144-28cb30516a83', 'filename': 'retail_sample.csv', 'status': 'completed', 'uploaded_at': '2025-07-31T12:23:11.958354', 'completed_at': '2025-07-31T12:23:11.986129', 'file_size': 710}], 'total_files': 4}\n",
      "Checking http://127.0.0.1:8000/api/v1/predict ...\n",
      "Status: 200\n",
      "Response: {'success': True, 'forecast': [{'ds': '2025-06-11', 'yhat': 273.9526875833591, 'yhat_lower': 273.952618667552, 'yhat_upper': 273.95275712767534}, {'ds': '2025-06-12', 'yhat': 83.22731900187956, 'yhat_lower': 83.22725543324638, 'yhat_upper': 83.22738397873425}, {'ds': '2025-06-13', 'yhat': -20.186496013788748, 'yhat_lower': -20.18652472883213, 'yhat_upper': -20.18646660504675}, {'ds': '2025-06-14', 'yhat': -120.17341471865208, 'yhat_lower': -120.17368593480387, 'yhat_upper': -120.17312894439016}, {'ds': '2025-06-15', 'yhat': -241.61003797956243, 'yhat_lower': -241.61080986381904, 'yhat_upper': -241.60923584137714}, {'ds': '2025-06-16', 'yhat': -181.934412975195, 'yhat_lower': -181.93516481659734, 'yhat_upper': -181.9336191824837}, {'ds': '2025-06-17', 'yhat': -143.23672605415862, 'yhat_lower': -143.23748329068857, 'yhat_upper': -143.2359527005984}], 'model_used': 'prophet', 'model_id': 'prophet_20250731_122759', 'selection_confidence': 1.0, 'training_metrics': {'mae': 0.00014848814977597158, 'rmse': 0.0001866013510025457, 'mape': 6.840243043211996e-05, 'r2': 0.9999999999569484}, 'forecast_periods': 7, 'frequency': 'daily', 'generated_at': '2025-07-31T12:27:59.782076', 'model_performance': {'mae': 0.00014848814977597158, 'rmse': 0.0001866013510025457, 'r2': 0.9999999999569484}}\n",
      "Checking http://127.0.0.1:8000/api/v1/models ...\n",
      "Status: 200\n",
      "Response: {'success': True, 'models': {'prophet': {'name': 'Prophet', 'description': \"Facebook's time series forecasting tool, excellent for seasonal data\", 'best_for': ['Strong seasonal patterns', 'Holiday effects', 'Missing data tolerance', 'Retail sales forecasting'], 'requirements': {'min_data_points': 60, 'seasonality': 'preferred', 'missing_data_tolerance': 'high'}, 'outputs': ['point_forecast', 'confidence_intervals', 'trend_components']}, 'xgboost': {'name': 'XGBoost', 'description': 'Gradient boosting for complex pattern recognition with multiple features', 'best_for': ['Multiple input features', 'Non-linear patterns', 'Feature importance analysis', 'Complex business rules'], 'requirements': {'min_data_points': 100, 'features': 'multiple_preferred', 'missing_data_tolerance': 'low'}, 'outputs': ['point_forecast', 'feature_importance', 'confidence_intervals']}, 'arima': {'name': 'ARIMA', 'description': 'Classical time series model for stationary data with autocorrelation', 'best_for': ['Stationary time series', 'Simple trend patterns', 'Quick forecasting', 'Traditional time series analysis'], 'requirements': {'min_data_points': 30, 'stationarity': 'preferred', 'missing_data_tolerance': 'very_low'}, 'outputs': ['point_forecast', 'confidence_intervals', 'model_diagnostics']}}, 'recommendation': 'Use auto-forecast endpoint for automatic model selection', 'selection_criteria': {'data_characteristics': 'Seasonality, trend, stationarity', 'data_quality': 'Completeness, consistency', 'feature_availability': 'Additional variables for XGBoost', 'business_context': 'Indian retail patterns and holidays'}}\n",
      "Checking http://127.0.0.1:8000/api/v1/models/compare/26afd863-745b-4151-8854-c4bb266b9636 ...\n",
      "Training status: completed\n",
      "Checking http://127.0.0.1:8000/api/v1/upload/list ...\n",
      "Status: 200\n",
      "Response: {'success': True, 'files': [{'file_id': '26afd863-745b-4151-8854-c4bb266b9636', 'filename': 'retail_sample.csv', 'status': 'completed', 'uploaded_at': '2025-07-31T12:27:46.979281', 'completed_at': '2025-07-31T12:27:47.006245', 'file_size': 710}, {'file_id': '99696596-c613-4992-8299-bdac25475181', 'filename': 'retail_sample.csv', 'status': 'completed', 'uploaded_at': '2025-07-31T12:26:25.686658', 'completed_at': '2025-07-31T12:26:25.725872', 'file_size': 710}, {'file_id': '47d3cab4-33d2-4c47-87df-b2d7603c9ffa', 'filename': 'retail_sample.csv', 'status': 'completed', 'uploaded_at': '2025-07-31T12:26:01.788723', 'completed_at': '2025-07-31T12:26:01.807646', 'file_size': 710}, {'file_id': '4bbf39b9-a48f-4865-9144-28cb30516a83', 'filename': 'retail_sample.csv', 'status': 'completed', 'uploaded_at': '2025-07-31T12:23:11.958354', 'completed_at': '2025-07-31T12:23:11.986129', 'file_size': 710}], 'total_files': 4}\n",
      "Checking http://127.0.0.1:8000/api/v1/predict ...\n",
      "Status: 200\n",
      "Response: {'success': True, 'forecast': [{'ds': '2025-06-11', 'yhat': 273.9526875833591, 'yhat_lower': 273.952618667552, 'yhat_upper': 273.95275712767534}, {'ds': '2025-06-12', 'yhat': 83.22731900187956, 'yhat_lower': 83.22725543324638, 'yhat_upper': 83.22738397873425}, {'ds': '2025-06-13', 'yhat': -20.186496013788748, 'yhat_lower': -20.18652472883213, 'yhat_upper': -20.18646660504675}, {'ds': '2025-06-14', 'yhat': -120.17341471865208, 'yhat_lower': -120.17368593480387, 'yhat_upper': -120.17312894439016}, {'ds': '2025-06-15', 'yhat': -241.61003797956243, 'yhat_lower': -241.61080986381904, 'yhat_upper': -241.60923584137714}, {'ds': '2025-06-16', 'yhat': -181.934412975195, 'yhat_lower': -181.93516481659734, 'yhat_upper': -181.9336191824837}, {'ds': '2025-06-17', 'yhat': -143.23672605415862, 'yhat_lower': -143.23748329068857, 'yhat_upper': -143.2359527005984}], 'model_used': 'prophet', 'model_id': 'prophet_20250731_122759', 'selection_confidence': 1.0, 'training_metrics': {'mae': 0.00014848814977597158, 'rmse': 0.0001866013510025457, 'mape': 6.840243043211996e-05, 'r2': 0.9999999999569484}, 'forecast_periods': 7, 'frequency': 'daily', 'generated_at': '2025-07-31T12:27:59.782076', 'model_performance': {'mae': 0.00014848814977597158, 'rmse': 0.0001866013510025457, 'r2': 0.9999999999569484}}\n",
      "Checking http://127.0.0.1:8000/api/v1/models ...\n",
      "Status: 200\n",
      "Response: {'success': True, 'models': {'prophet': {'name': 'Prophet', 'description': \"Facebook's time series forecasting tool, excellent for seasonal data\", 'best_for': ['Strong seasonal patterns', 'Holiday effects', 'Missing data tolerance', 'Retail sales forecasting'], 'requirements': {'min_data_points': 60, 'seasonality': 'preferred', 'missing_data_tolerance': 'high'}, 'outputs': ['point_forecast', 'confidence_intervals', 'trend_components']}, 'xgboost': {'name': 'XGBoost', 'description': 'Gradient boosting for complex pattern recognition with multiple features', 'best_for': ['Multiple input features', 'Non-linear patterns', 'Feature importance analysis', 'Complex business rules'], 'requirements': {'min_data_points': 100, 'features': 'multiple_preferred', 'missing_data_tolerance': 'low'}, 'outputs': ['point_forecast', 'feature_importance', 'confidence_intervals']}, 'arima': {'name': 'ARIMA', 'description': 'Classical time series model for stationary data with autocorrelation', 'best_for': ['Stationary time series', 'Simple trend patterns', 'Quick forecasting', 'Traditional time series analysis'], 'requirements': {'min_data_points': 30, 'stationarity': 'preferred', 'missing_data_tolerance': 'very_low'}, 'outputs': ['point_forecast', 'confidence_intervals', 'model_diagnostics']}}, 'recommendation': 'Use auto-forecast endpoint for automatic model selection', 'selection_criteria': {'data_characteristics': 'Seasonality, trend, stationarity', 'data_quality': 'Completeness, consistency', 'feature_availability': 'Additional variables for XGBoost', 'business_context': 'Indian retail patterns and holidays'}}\n",
      "Checking http://127.0.0.1:8000/api/v1/models/compare/26afd863-745b-4151-8854-c4bb266b9636 ...\n",
      "Error accessing http://127.0.0.1:8000/api/v1/models/compare/26afd863-745b-4151-8854-c4bb266b9636: HTTPConnectionPool(host='127.0.0.1', port=8000): Read timed out. (read timeout=10)\n",
      "Error accessing http://127.0.0.1:8000/api/v1/models/compare/26afd863-745b-4151-8854-c4bb266b9636: HTTPConnectionPool(host='127.0.0.1', port=8000): Read timed out. (read timeout=10)\n"
     ]
    }
   ],
   "source": [
    "# Verify All API Endpoints Respond Correctly with Real Data\n",
    "import requests, time\n",
    "base_url = 'http://127.0.0.1:8000'\n",
    "sample_file = 'data/samples/retail_sample.csv'\n",
    "files = {'file': open(sample_file, 'rb')}\n",
    "\n",
    "# Upload file\n",
    "response = requests.post(f'{base_url}/api/v1/upload', files=files)\n",
    "try:\n",
    "    resp_json = response.json()\n",
    "except Exception:\n",
    "    print('Upload failed. Response:', response.text)\n",
    "    raise\n",
    "file_id = resp_json.get('file_id') or (resp_json.get('data', {}) or {}).get('file_id')\n",
    "print('Uploaded file_id:', file_id)\n",
    "if not file_id:\n",
    "    print('Upload failed. Response:', resp_json)\n",
    "    raise Exception('Upload failed, no file_id returned')\n",
    "\n",
    "# Poll for processing completion\n",
    "list_url = f'{base_url}/api/v1/upload/list'\n",
    "status = None\n",
    "for _ in range(30):\n",
    "    response = requests.get(list_url)\n",
    "    result = response.json()\n",
    "    status = next((f['status'] for f in result['files'] if f['file_id'] == file_id), None)\n",
    "    print('Processing status:', status)\n",
    "    if status == 'completed':\n",
    "        break\n",
    "    time.sleep(2)\n",
    "if status != 'completed':\n",
    "    print('File processing did not complete.')\n",
    "    raise Exception('Processing failed')\n",
    "\n",
    "# Train model (Prophet)\n",
    "train_url = f'{base_url}/api/v1/train'\n",
    "train_req = {'file_id': file_id, 'model_type': 'prophet'}\n",
    "response = requests.post(train_url, json=train_req)\n",
    "train_json = response.json()\n",
    "job_id = train_json.get('job_id')\n",
    "print('Training job_id:', job_id)\n",
    "\n",
    "# Poll for training completion\n",
    "status_url = f'{base_url}/api/v1/train/status/{job_id}'\n",
    "train_status = None\n",
    "for _ in range(30):\n",
    "    response = requests.get(status_url)\n",
    "    train_status = response.json()\n",
    "    print('Training status:', train_status.get('status'))\n",
    "    if train_status.get('status') == 'completed':\n",
    "        break\n",
    "    if train_status.get('status') == 'failed':\n",
    "        print('Training failed:', train_status.get('error'))\n",
    "        raise Exception('Training failed')\n",
    "    time.sleep(2)\n",
    "if train_status.get('status') != 'completed':\n",
    "    print('Model training did not complete.')\n",
    "    raise Exception('Training failed')\n",
    "\n",
    "# Endpoints to verify\n",
    "endpoints = [\n",
    "    ('/api/v1/upload/list', 'get'),\n",
    "    ('/api/v1/predict', 'post', {'file_id': file_id, 'model_type': 'prophet', 'forecast_periods': 7}),\n",
    "    ('/api/v1/models', 'get'),\n",
    "    (f'/api/v1/models/compare/{file_id}', 'get'),\n",
    "]\n",
    "for ep in endpoints:\n",
    "    url = base_url + ep[0]\n",
    "    method = ep[1]\n",
    "    print(f'Checking {url} ...')\n",
    "    try:\n",
    "        if method == 'get':\n",
    "            response = requests.get(url, timeout=10)\n",
    "        elif method == 'post':\n",
    "            response = requests.post(url, json=ep[2], timeout=10)\n",
    "        print('Status:', response.status_code)\n",
    "        print('Response:', response.json() if 'application/json' in response.headers.get('content-type','') else response.text[:500])\n",
    "    except Exception as e:\n",
    "        print(f'Error accessing {url}:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3063e711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No uploaded files found.\n"
     ]
    }
   ],
   "source": [
    "# Get a valid file_id from uploaded files and use for forecast endpoints\n",
    "list_url = 'http://127.0.0.1:8000/api/v1/upload/list'\n",
    "response = requests.get(list_url)\n",
    "files = response.json().get('files', [])\n",
    "file_id = None\n",
    "if files:\n",
    "    file_id = files[0]['file_id']\n",
    "    print(f'Using file_id: {file_id}')\n",
    "else:\n",
    "    print('No uploaded files found.')\n",
    "\n",
    "if file_id:\n",
    "    endpoints = [\n",
    "        ('/api/v1/predict', 'post', {'file_id': file_id, 'model_type': 'prophet', 'forecast_periods': 7}),\n",
    "        (f'/api/v1/models/compare/{file_id}', 'get'),\n",
    "    ]\n",
    "    base_url = 'http://127.0.0.1:8000'\n",
    "    for ep in endpoints:\n",
    "        url = base_url + ep[0]\n",
    "        method = ep[1]\n",
    "        print(f'Checking {url} ...')\n",
    "        try:\n",
    "            if method == 'get':\n",
    "                response = requests.get(url, timeout=30)\n",
    "            elif method == 'post':\n",
    "                response = requests.post(url, json=ep[2], timeout=30)\n",
    "            print('Status:', response.status_code)\n",
    "            print('Response:', response.json() if 'application/json' in response.headers.get('content-type','') else response.text[:500])\n",
    "        except Exception as e:\n",
    "            print(f'Error accessing {url}:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "594319d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload response: 202\n",
      "Uploaded file_id: 72b3ea82-f8bb-4fb9-9dab-b5be76da029f\n",
      "Check 1: completed\n",
      "File processing completed.\n",
      "Checking http://127.0.0.1:8000/api/v1/predict ...\n",
      "Error accessing http://127.0.0.1:8000/api/v1/predict: HTTPConnectionPool(host='127.0.0.1', port=8000): Read timed out. (read timeout=30)\n",
      "Checking http://127.0.0.1:8000/api/v1/models/compare/72b3ea82-f8bb-4fb9-9dab-b5be76da029f ...\n",
      "Error accessing http://127.0.0.1:8000/api/v1/predict: HTTPConnectionPool(host='127.0.0.1', port=8000): Read timed out. (read timeout=30)\n",
      "Checking http://127.0.0.1:8000/api/v1/models/compare/72b3ea82-f8bb-4fb9-9dab-b5be76da029f ...\n",
      "Error accessing http://127.0.0.1:8000/api/v1/models/compare/72b3ea82-f8bb-4fb9-9dab-b5be76da029f: HTTPConnectionPool(host='127.0.0.1', port=8000): Read timed out. (read timeout=30)\n",
      "Error accessing http://127.0.0.1:8000/api/v1/models/compare/72b3ea82-f8bb-4fb9-9dab-b5be76da029f: HTTPConnectionPool(host='127.0.0.1', port=8000): Read timed out. (read timeout=30)\n"
     ]
    }
   ],
   "source": [
    "# Full workflow: upload, poll, train, poll, forecast\n",
    "import requests, time\n",
    "base_url = \"http://localhost:8000/api/v1\"\n",
    "sample_file = \"data/samples/retail_sample.csv\"\n",
    "files = {\"file\": open(sample_file, \"rb\")}\n",
    "\n",
    "# Upload file\n",
    "response = requests.post(f\"{base_url}/upload\", files=files)\n",
    "resp_json = response.json()\n",
    "file_id = resp_json.get(\"file_id\")\n",
    "print(\"Uploaded file_id:\", file_id)\n",
    "\n",
    "# Poll for processing completion\n",
    "list_url = f\"{base_url}/upload/list\"\n",
    "status = None\n",
    "for _ in range(30):\n",
    "    response = requests.get(list_url)\n",
    "    result = response.json()\n",
    "    status = next((f[\"status\"] for f in result[\"files\"] if f[\"file_id\"] == file_id), None)\n",
    "    print(\"Processing status:\", status)\n",
    "    if status == \"completed\":\n",
    "        break\n",
    "    time.sleep(2)\n",
    "if status != \"completed\":\n",
    "    print(\"File processing did not complete.\")\n",
    "    raise Exception(\"Processing failed\")\n",
    "\n",
    "# Train model (Prophet)\n",
    "train_url = f\"{base_url}/train\"\n",
    "train_req = {\"file_id\": file_id, \"model_type\": \"prophet\"}\n",
    "response = requests.post(train_url, json=train_req)\n",
    "train_json = response.json()\n",
    "job_id = train_json.get(\"job_id\")\n",
    "print(\"Training job_id:\", job_id)\n",
    "\n",
    "# Poll for training completion\n",
    "status_url = f\"{base_url}/train/status/{job_id}\"\n",
    "train_status = None\n",
    "for _ in range(30):\n",
    "    response = requests.get(status_url)\n",
    "    train_status = response.json()\n",
    "    print(\"Training status:\", train_status.get(\"status\"))\n",
    "    if train_status.get(\"status\") == \"completed\":\n",
    "        break\n",
    "    if train_status.get(\"status\") == \"failed\":\n",
    "        print(\"Training failed:\", train_status.get(\"error\"))\n",
    "        raise Exception(\"Training failed\")\n",
    "    time.sleep(2)\n",
    "if train_status.get(\"status\") != \"completed\":\n",
    "    print(\"Model training did not complete.\")\n",
    "    raise Exception(\"Training failed\")\n",
    "\n",
    "# Forecast\n",
    "predict_url = f\"{base_url}/predict\"\n",
    "predict_req = {\"file_id\": file_id, \"model_type\": \"prophet\", \"forecast_periods\": 30}\n",
    "response = requests.post(predict_url, json=predict_req)\n",
    "predict_json = response.json()\n",
    "if not predict_json.get(\"success\"):\n",
    "    print(\"Forecast error:\", predict_json.get(\"message\", predict_json.get(\"error\")))\n",
    "else:\n",
    "    print(\"Forecast result:\")\n",
    "    print(predict_json[\"forecast\"][:5])  # Show first 5 forecast rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d8793688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict status: 500\n",
      "Predict response: {'detail': 'Object of type Timestamp is not JSON serializable'}\n",
      "Error accessing /models/compare: HTTPConnectionPool(host='127.0.0.1', port=8000): Read timed out. (read timeout=15)\n",
      "Error accessing /models/compare: HTTPConnectionPool(host='127.0.0.1', port=8000): Read timed out. (read timeout=15)\n"
     ]
    }
   ],
   "source": [
    "# Test /predict and /models/compare endpoints with provided file ID\n",
    "import requests\n",
    "base_url = 'http://127.0.0.1:8000/api/v1'\n",
    "file_id = '48649579-6764-4fcc-939e-e9f3cca93514'\n",
    "\n",
    "# Test /predict\n",
    "predict_url = f'{base_url}/predict'\n",
    "predict_req = {'file_id': file_id, 'model_type': 'prophet', 'forecast_periods': 7}\n",
    "try:\n",
    "    response = requests.post(predict_url, json=predict_req, timeout=15)\n",
    "    print('Predict status:', response.status_code)\n",
    "    if 'application/json' in response.headers.get('content-type',''):\n",
    "        print('Predict response:', response.json())\n",
    "    else:\n",
    "        print('Predict response:', response.text[:500])\n",
    "except Exception as e:\n",
    "    print('Error accessing /predict:', e)\n",
    "\n",
    "# Test /models/compare\n",
    "compare_url = f'{base_url}/models/compare/{file_id}'\n",
    "try:\n",
    "    response = requests.get(compare_url, timeout=15)\n",
    "    print('Compare status:', response.status_code)\n",
    "    if 'application/json' in response.headers.get('content-type',''):\n",
    "        print('Compare response:', response.json())\n",
    "    else:\n",
    "        print('Compare response:', response.text[:500])\n",
    "except Exception as e:\n",
    "    print('Error accessing /models/compare:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5a5cdbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File List Status: 200\n",
      "File List Response: {'success': True, 'files': [], 'total_files': 0}\n",
      "File not found: 48649579-6764-4fcc-939e-e9f3cca93514\n",
      "Predict Status: 500\n",
      "Predict Response: {'detail': ''}\n",
      "Auto-Forecast Status: 422\n",
      "Auto-Forecast Response: {'detail': [{'type': 'missing', 'loc': ['query', 'file_id'], 'msg': 'Field required', 'input': None, 'url': 'https://errors.pydantic.dev/2.5/v/missing'}]}\n",
      "Compare Status: 404\n",
      "Compare Response: {'error': 'Endpoint not found', 'status_code': 404}\n"
     ]
    }
   ],
   "source": [
    "# Practical API Test Client for Forecast Endpoints\n",
    "import requests\n",
    "\n",
    "class ForecastAPITester:\n",
    "    def __init__(self, base_url=\"http://127.0.0.1:8000/api/v1\"):\n",
    "        self.base_url = base_url\n",
    "\n",
    "    def get_file_list(self):\n",
    "        url = f\"{self.base_url}/upload/list\"\n",
    "        resp = requests.get(url)\n",
    "        print(\"File List Status:\", resp.status_code)\n",
    "        print(\"File List Response:\", resp.json())\n",
    "        return resp.json().get(\"files\", [])\n",
    "\n",
    "    def get_file_info(self, file_id):\n",
    "        files = self.get_file_list()\n",
    "        for f in files:\n",
    "            if f[\"file_id\"] == file_id:\n",
    "                print(\"File Info:\", f)\n",
    "                return f\n",
    "        print(\"File not found:\", file_id)\n",
    "        return None\n",
    "\n",
    "    def test_predict(self, file_id, model_type=\"prophet\", forecast_periods=7, frequency=\"daily\"):\n",
    "        url = f\"{self.base_url}/predict\"\n",
    "        payload = {\n",
    "            \"file_id\": file_id,\n",
    "            \"model_type\": model_type,\n",
    "            \"forecast_periods\": forecast_periods,\n",
    "            \"frequency\": frequency\n",
    "        }\n",
    "        resp = requests.post(url, json=payload)\n",
    "        print(\"Predict Status:\", resp.status_code)\n",
    "        try:\n",
    "            print(\"Predict Response:\", resp.json())\n",
    "        except Exception:\n",
    "            print(\"Predict Response (raw):\", resp.text[:500])\n",
    "\n",
    "    def test_auto_forecast(self, file_id, forecast_periods=7):\n",
    "        url = f\"{self.base_url}/auto-forecast\"\n",
    "        payload = {\n",
    "            \"file_id\": file_id,\n",
    "            \"forecast_periods\": forecast_periods\n",
    "        }\n",
    "        resp = requests.post(url, json=payload)\n",
    "        print(\"Auto-Forecast Status:\", resp.status_code)\n",
    "        try:\n",
    "            print(\"Auto-Forecast Response:\", resp.json())\n",
    "        except Exception:\n",
    "            print(\"Auto-Forecast Response (raw):\", resp.text[:500])\n",
    "\n",
    "    def test_compare(self, file_id):\n",
    "        url = f\"{self.base_url}/models/compare/{file_id}\"\n",
    "        resp = requests.get(url)\n",
    "        print(\"Compare Status:\", resp.status_code)\n",
    "        try:\n",
    "            print(\"Compare Response:\", resp.json())\n",
    "        except Exception:\n",
    "            print(\"Compare Response (raw):\", resp.text[:500])\n",
    "\n",
    "# Usage Example:\n",
    "tester = ForecastAPITester()\n",
    "file_id = \"48649579-6764-4fcc-939e-e9f3cca93514\"  # Replace with your actual file_id\n",
    "tester.get_file_info(file_id)\n",
    "tester.test_predict(file_id)\n",
    "tester.test_auto_forecast(file_id)\n",
    "tester.test_compare(file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65b8330b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file_id: 2ac15af2-19f3-44f9-be28-4466a8f4d8b5\n",
      "Processing status: completed\n",
      "Training job_id: f532c5af-7293-49c7-a3dd-e9bf3ca7f7db\n",
      "Training status: completed\n",
      "\n",
      "--- Endpoint Tests ---\n",
      "Predict Status: 200\n",
      "Predict Response: {'success': True, 'forecast': [{'ds': '2025-06-11', 'yhat': 273.9526875833591, 'yhat_lower': 273.9526087608748, 'yhat_upper': 273.952756313504}, {'ds': '2025-06-12', 'yhat': 83.22731900187956, 'yhat_lower': 83.22724537302477, 'yhat_upper': 83.22738485418813}, {'ds': '2025-06-13', 'yhat': -20.186496013788748, 'yhat_lower': -20.186527097745326, 'yhat_upper': -20.186461713600902}, {'ds': '2025-06-14', 'yhat': -120.17341471865208, 'yhat_lower': -120.17369180462909, 'yhat_upper': -120.17310225949133}, {'ds': '2025-06-15', 'yhat': -241.61003797956243, 'yhat_lower': -241.61083911936743, 'yhat_upper': -241.6091697748569}, {'ds': '2025-06-16', 'yhat': -181.934412975195, 'yhat_lower': -181.9351989730114, 'yhat_upper': -181.93360141041822}, {'ds': '2025-06-17', 'yhat': -143.23672605415862, 'yhat_lower': -143.2374868621093, 'yhat_upper': -143.23594947453347}], 'model_used': 'prophet', 'model_id': 'prophet_20250731_113711', 'selection_confidence': 1.0, 'training_metrics': {'mae': 0.00014848814977597158, 'rmse': 0.0001866013510025457, 'mape': 6.840243043211996e-05, 'r2': 0.9999999999569484}, 'forecast_periods': 7, 'frequency': 'daily', 'generated_at': '2025-07-31T11:37:11.388746', 'model_performance': {'mae': 0.00014848814977597158, 'rmse': 0.0001866013510025457, 'r2': 0.9999999999569484}}\n",
      "Auto-Forecast Status: 405\n",
      "Auto-Forecast Response: {'detail': 'Method Not Allowed'}\n",
      "Training status: completed\n",
      "\n",
      "--- Endpoint Tests ---\n",
      "Predict Status: 200\n",
      "Predict Response: {'success': True, 'forecast': [{'ds': '2025-06-11', 'yhat': 273.9526875833591, 'yhat_lower': 273.9526087608748, 'yhat_upper': 273.952756313504}, {'ds': '2025-06-12', 'yhat': 83.22731900187956, 'yhat_lower': 83.22724537302477, 'yhat_upper': 83.22738485418813}, {'ds': '2025-06-13', 'yhat': -20.186496013788748, 'yhat_lower': -20.186527097745326, 'yhat_upper': -20.186461713600902}, {'ds': '2025-06-14', 'yhat': -120.17341471865208, 'yhat_lower': -120.17369180462909, 'yhat_upper': -120.17310225949133}, {'ds': '2025-06-15', 'yhat': -241.61003797956243, 'yhat_lower': -241.61083911936743, 'yhat_upper': -241.6091697748569}, {'ds': '2025-06-16', 'yhat': -181.934412975195, 'yhat_lower': -181.9351989730114, 'yhat_upper': -181.93360141041822}, {'ds': '2025-06-17', 'yhat': -143.23672605415862, 'yhat_lower': -143.2374868621093, 'yhat_upper': -143.23594947453347}], 'model_used': 'prophet', 'model_id': 'prophet_20250731_113711', 'selection_confidence': 1.0, 'training_metrics': {'mae': 0.00014848814977597158, 'rmse': 0.0001866013510025457, 'mape': 6.840243043211996e-05, 'r2': 0.9999999999569484}, 'forecast_periods': 7, 'frequency': 'daily', 'generated_at': '2025-07-31T11:37:11.388746', 'model_performance': {'mae': 0.00014848814977597158, 'rmse': 0.0001866013510025457, 'r2': 0.9999999999569484}}\n",
      "Auto-Forecast Status: 405\n",
      "Auto-Forecast Response: {'detail': 'Method Not Allowed'}\n",
      "Compare Status: 200\n",
      "Compare Response: {'success': True, 'file_id': '2ac15af2-19f3-44f9-be28-4466a8f4d8b5', 'comparison': {'successful_models': [{'model': 'prophet', 'mae': 0.00014848814977597158, 'rmse': 0.0001866013510025457, 'r2': 0.9999999999569484, 'training_time': 'N/A', 'suitable_for': ['Seasonal retail patterns', 'Holiday impact analysis', 'Missing data handling', 'Long-term trend forecasting']}], 'failed_models': [], 'recommendation': {'model': 'prophet', 'mae': 0.00014848814977597158, 'rmse': 0.0001866013510025457, 'r2': 0.9999999999569484, 'training_time': 'N/A', 'suitable_for': ['Seasonal retail patterns', 'Holiday impact analysis', 'Missing data handling', 'Long-term trend forecasting']}}, 'detailed_results': {'prophet': {'success': True, 'model_id': 'prophet_20250731_113723', 'metrics': {'mae': 0.00014848814977597158, 'rmse': 0.0001866013510025457, 'mape': 6.840243043211996e-05, 'r2': 0.9999999999569484}}}, 'model_ranking': ['prophet_20250731_113723'], 'generated_at': '2025-07-31T11:37:23.827701'}\n",
      "Compare Status: 200\n",
      "Compare Response: {'success': True, 'file_id': '2ac15af2-19f3-44f9-be28-4466a8f4d8b5', 'comparison': {'successful_models': [{'model': 'prophet', 'mae': 0.00014848814977597158, 'rmse': 0.0001866013510025457, 'r2': 0.9999999999569484, 'training_time': 'N/A', 'suitable_for': ['Seasonal retail patterns', 'Holiday impact analysis', 'Missing data handling', 'Long-term trend forecasting']}], 'failed_models': [], 'recommendation': {'model': 'prophet', 'mae': 0.00014848814977597158, 'rmse': 0.0001866013510025457, 'r2': 0.9999999999569484, 'training_time': 'N/A', 'suitable_for': ['Seasonal retail patterns', 'Holiday impact analysis', 'Missing data handling', 'Long-term trend forecasting']}}, 'detailed_results': {'prophet': {'success': True, 'model_id': 'prophet_20250731_113723', 'metrics': {'mae': 0.00014848814977597158, 'rmse': 0.0001866013510025457, 'mape': 6.840243043211996e-05, 'r2': 0.9999999999569484}}}, 'model_ranking': ['prophet_20250731_113723'], 'generated_at': '2025-07-31T11:37:23.827701'}\n"
     ]
    }
   ],
   "source": [
    "# Automated workflow: upload, poll, extract file_id, and test all endpoints\n",
    "import requests, time\n",
    "base_url = 'http://127.0.0.1:8000/api/v1'\n",
    "sample_file = 'data/samples/retail_sample.csv'\n",
    "files = {'file': open(sample_file, 'rb')}\n",
    "\n",
    "# Upload file\n",
    "response = requests.post(f'{base_url}/upload', files=files)\n",
    "resp_json = response.json()\n",
    "file_id = resp_json.get('file_id') or (resp_json.get('data', {}) or {}).get('file_id')\n",
    "print('Uploaded file_id:', file_id)\n",
    "if not file_id:\n",
    "    print('Upload failed. Response:', resp_json)\n",
    "    raise Exception('Upload failed, no file_id returned')\n",
    "\n",
    "# Poll for processing completion\n",
    "list_url = f'{base_url}/upload/list'\n",
    "status = None\n",
    "for _ in range(30):\n",
    "    response = requests.get(list_url)\n",
    "    result = response.json()\n",
    "    status = next((f['status'] for f in result['files'] if f['file_id'] == file_id), None)\n",
    "    print('Processing status:', status)\n",
    "    if status == 'completed':\n",
    "        break\n",
    "    time.sleep(2)\n",
    "if status != 'completed':\n",
    "    print('File processing did not complete.')\n",
    "    raise Exception('Processing failed')\n",
    "\n",
    "# Train model (Prophet)\n",
    "train_url = f'{base_url}/train'\n",
    "train_req = {'file_id': file_id, 'model_type': 'prophet'}\n",
    "response = requests.post(train_url, json=train_req)\n",
    "train_json = response.json()\n",
    "job_id = train_json.get('job_id')\n",
    "print('Training job_id:', job_id)\n",
    "\n",
    "# Poll for training completion\n",
    "status_url = f'{base_url}/train/status/{job_id}'\n",
    "train_status = None\n",
    "for _ in range(30):\n",
    "    response = requests.get(status_url)\n",
    "    train_status = response.json()\n",
    "    print('Training status:', train_status.get('status'))\n",
    "    if train_status.get('status') == 'completed':\n",
    "        break\n",
    "    if train_status.get('status') == 'failed':\n",
    "        print('Training failed:', train_status.get('error'))\n",
    "        raise Exception('Training failed')\n",
    "    time.sleep(2)\n",
    "if train_status.get('status') != 'completed':\n",
    "    print('Model training did not complete.')\n",
    "    raise Exception('Training failed')\n",
    "\n",
    "# Test endpoints with valid file_id\n",
    "print('\\n--- Endpoint Tests ---')\n",
    "# /predict\n",
    "predict_url = f'{base_url}/predict'\n",
    "predict_req = {'file_id': file_id, 'model_type': 'prophet', 'forecast_periods': 7}\n",
    "response = requests.post(predict_url, json=predict_req)\n",
    "print('Predict Status:', response.status_code)\n",
    "print('Predict Response:', response.json() if 'application/json' in response.headers.get('content-type','') else response.text[:500])\n",
    "# /auto-forecast\n",
    "auto_url = f'{base_url}/auto-forecast?file_id={file_id}&forecast_periods=7'\n",
    "response = requests.get(auto_url)\n",
    "print('Auto-Forecast Status:', response.status_code)\n",
    "print('Auto-Forecast Response:', response.json() if 'application/json' in response.headers.get('content-type','') else response.text[:500])\n",
    "# /models/compare\n",
    "compare_url = f'{base_url}/models/compare/{file_id}'\n",
    "response = requests.get(compare_url)\n",
    "print('Compare Status:', response.status_code)\n",
    "print('Compare Response:', response.json() if 'application/json' in response.headers.get('content-type','') else response.text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dff89d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test auto-forecast endpoint with GET and POST methods\n",
    "import requests\n",
    "\n",
    "def test_auto_forecast_methods(file_id):\n",
    "    base_url = \"http://127.0.0.1:8000/api/v1\"\n",
    "    url = f\"{base_url}/auto-forecast?file_id={file_id}&forecast_periods=7\"\n",
    "    # Test GET\n",
    "    resp = requests.get(url)\n",
    "    print(\"GET Status:\", resp.status_code)\n",
    "    print(\"GET Response:\", resp.json() if 'application/json' in resp.headers.get('content-type','') else resp.text[:500])\n",
    "    # Test POST\n",
    "    resp = requests.post(url)\n",
    "    print(\"POST Status:\", resp.status_code)\n",
    "    print(\"POST Response:\", resp.json() if 'application/json' in resp.headers.get('content-type','') else resp.text[:500])\n",
    "\n",
    "# Usage: using the provided file_id\n",
    "test_auto_forecast_methods(\"2ac15af2-19f3-44f9-be28-4466a8f4d8b5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7710fcd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest file_id: 2ac15af2-19f3-44f9-be28-4466a8f4d8b5\n",
      "File info: {'file_id': '2ac15af2-19f3-44f9-be28-4466a8f4d8b5', 'filename': 'retail_sample.csv', 'status': 'completed', 'uploaded_at': '2025-07-31T11:36:58.782776', 'completed_at': '2025-07-31T11:36:58.865323', 'file_size': 710}\n"
     ]
    }
   ],
   "source": [
    "# Print the latest file_id from the upload list for endpoint testing\n",
    "import requests\n",
    "base_url = \"http://127.0.0.1:8000/api/v1\"\n",
    "list_url = f\"{base_url}/upload/list\"\n",
    "response = requests.get(list_url)\n",
    "files = response.json().get(\"files\", [])\n",
    "if files:\n",
    "    latest_file = files[0]\n",
    "    print(\"Latest file_id:\", latest_file[\"file_id\"])\n",
    "    print(\"File info:\", latest_file)\n",
    "else:\n",
    "    print(\"No files found. Please upload a file first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfab0132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Python dependencies...\n",
      "Dependencies: OK\n",
      "\n",
      "Running tests...\n",
      "Tests: \n",
      "\n",
      "Checking server status...\n",
      "Server: OK\n",
      "\n",
      "Checking API docs...\n",
      "API Docs: OK\n",
      "\n",
      "Checking sample data generation...\n",
      "Dependencies: OK\n",
      "\n",
      "Running tests...\n",
      "Tests: \n",
      "\n",
      "Checking server status...\n",
      "Server: OK\n",
      "\n",
      "Checking API docs...\n",
      "API Docs: OK\n",
      "\n",
      "Checking sample data generation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-25 (_readerthread):\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"d:\\retail-intelligence\\retail_env\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 772, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\subprocess.py\", line 1499, in _readerthread\n",
      "    buffer.append(fh.read())\n",
      "  File \"C:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\encodings\\cp1252.py\", line 23, in decode\n",
      "    return codecs.charmap_decode(input,self.errors,decoding_table)[0]\n",
      "UnicodeDecodeError: 'charmap' codec can't decode byte 0x8d in position 187: character maps to <undefined>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Data: OK\n",
      "\n",
      "Checking file upload...\n",
      "File Upload: Status 202\n",
      "\n",
      "Checking endpoint responses...\n",
      "/api/v1/upload/list: OK\n",
      "/api/v1/models: OK\n"
     ]
    }
   ],
   "source": [
    "# Platform Health Check Summary\n",
    "import subprocess, requests, os\n",
    "\n",
    "print('Checking Python dependencies...')\n",
    "try:\n",
    "    result = subprocess.run(['pip', 'check'], capture_output=True, text=True)\n",
    "    print('Dependencies:', 'OK' if result.returncode == 0 else result.stdout)\n",
    "except Exception as e:\n",
    "    print('Dependency check error:', e)\n",
    "\n",
    "print('\\nRunning tests...')\n",
    "try:\n",
    "    result = subprocess.run(['python', 'test_platfrom.py'], capture_output=True, text=True)\n",
    "    print('Tests:', 'OK' if result.returncode == 0 else result.stdout)\n",
    "except Exception as e:\n",
    "    print('Test run error:', e)\n",
    "\n",
    "print('\\nChecking server status...')\n",
    "try:\n",
    "    resp = requests.get('http://127.0.0.1:8000')\n",
    "    print('Server:', 'OK' if resp.status_code == 200 else f'Status {resp.status_code}')\n",
    "except Exception as e:\n",
    "    print('Server error:', e)\n",
    "\n",
    "print('\\nChecking API docs...')\n",
    "try:\n",
    "    resp = requests.get('http://127.0.0.1:8000/docs')\n",
    "    print('API Docs:', 'OK' if resp.status_code == 200 else f'Status {resp.status_code}')\n",
    "except Exception as e:\n",
    "    print('Docs error:', e)\n",
    "\n",
    "print('\\nChecking sample data generation...')\n",
    "try:\n",
    "    result = subprocess.run(['python', 'generate_sample_data.py'], capture_output=True, text=True)\n",
    "    print('Sample Data:', 'OK' if result.returncode == 0 else result.stdout)\n",
    "except Exception as e:\n",
    "    print('Sample data error:', e)\n",
    "\n",
    "print('\\nChecking file upload...')\n",
    "try:\n",
    "    sample_file = 'data/samples/retail_sample.csv'\n",
    "    files = {'file': open(sample_file, 'rb')}\n",
    "    resp = requests.post('http://127.0.0.1:8000/api/v1/upload', files=files)\n",
    "    print('File Upload:', 'OK' if resp.status_code == 200 else f'Status {resp.status_code}')\n",
    "except Exception as e:\n",
    "    print('File upload error:', e)\n",
    "\n",
    "print('\\nChecking endpoint responses...')\n",
    "endpoints = [\n",
    "    '/api/v1/upload/list',\n",
    "    '/api/v1/models',\n",
    "]\n",
    "for ep in endpoints:\n",
    "    try:\n",
    "        resp = requests.get(f'http://127.0.0.1:8000{ep}')\n",
    "        print(f'{ep}:', 'OK' if resp.status_code == 200 else f'Status {resp.status_code}')\n",
    "    except Exception as e:\n",
    "        print(f'{ep} error:', e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retail_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
