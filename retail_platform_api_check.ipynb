{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1eeeb60",
   "metadata": {},
   "source": [
    "# Retail Intelligence Platform: System & API Health Check\n",
    "\n",
    "This notebook verifies the platform setup and API functionality step-by-step.\n",
    "\n",
    "**Checklist:**\n",
    "1. Check all dependencies installed without errors\n",
    "2. Run all tests in `test_platform.py`\n",
    "3. Start server and verify running at http://127.0.0.1:8000\n",
    "4. Access API documentation at http://127.0.0.1:8000/docs\n",
    "5. Generate sample data and verify success\n",
    "6. Test file upload functionality\n",
    "7. Verify all API endpoints respond correctly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f4055f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed packages:\n",
      "aiofiles==23.2.1\n",
      "annotated-types==0.7.0\n",
      "anyio==3.7.1\n",
      "asttokens==3.0.0\n",
      "Babel==2.13.1\n",
      "black==23.11.0\n",
      "certifi==2025.7.14\n",
      "charset-normalizer==3.4.2\n",
      "click==8.2.1\n",
      "cmdstanpy==1.2.5\n",
      "colorama==0.4.6\n",
      "comm==0.2.3\n",
      "contourpy==1.3.2\n",
      "convertdate==2.4.0\n",
      "cycler==0.12.1\n",
      "debugpy==1.8.15\n",
      "decorator==5.2.1\n",
      "ephem==4.2\n",
      "et_xmlfile==2.0.0\n",
      "exceptiongroup==1.3.0\n",
      "executing==2.2.0\n",
      "fastapi==0.104.1\n",
      "flake8==6.1.0\n",
      "fonttools==4.59.0\n",
      "greenlet==3.2.3\n",
      "h11==0.16.0\n",
      "holidays==0.77\n",
      "httpcore==1.0.9\n",
      "httptools==0.6.4\n",
      "httpx==0.25.2\n",
      "idna==3.10\n",
      "importlib_resources==6.5.2\n",
      "iniconfig==2.1.0\n",
      "ipykernel==6.30.0\n",
      "ipython==8.37.0\n",
      "jedi==0.19.2\n",
      "Jinja2==3.1.2\n",
      "joblib==1.5.1\n",
      "jupyter_client==8.6.3\n",
      "jupyter_core==5.8.1\n",
      "kiwisolver==1.4.8\n",
      "LunarCalendar==0.0.9\n",
      "MarkupSafe==3.0.2\n",
      "matplotlib==3.8.2\n",
      "matplotlib-inline==0.1.7\n",
      "mccabe==0.7.0\n",
      "mypy_extensions==1.1.0\n",
      "nest-asyncio==1.6.0\n",
      "numpy==1.24.3\n",
      "openpyxl==3.1.2\n",
      "packaging==25.0\n",
      "pandas==2.1.3\n",
      "parso==0.8.4\n",
      "pathspec==0.12.1\n",
      "patsy==1.0.1\n",
      "pillow==11.3.0\n",
      "platformdirs==4.3.8\n",
      "plotly==5.17.0\n",
      "pluggy==1.6.0\n",
      "prompt_toolkit==3.0.51\n",
      "prophet==1.1.4\n",
      "psutil==7.0.0\n",
      "pure_eval==0.2.3\n",
      "pycodestyle==2.11.1\n",
      "pydantic==2.5.0\n",
      "pydantic_core==2.14.1\n",
      "pyflakes==3.1.0\n",
      "Pygments==2.19.2\n",
      "PyMeeus==0.5.12\n",
      "pyparsing==3.2.3\n",
      "pytest==7.4.3\n",
      "pytest-asyncio==0.21.1\n",
      "python-dateutil==2.9.0.post0\n",
      "python-dotenv==1.0.0\n",
      "python-multipart==0.0.6\n",
      "pytz==2025.2\n",
      "pywin32==311\n",
      "PyYAML==6.0.2\n",
      "pyzmq==27.0.0\n",
      "requests==2.32.4\n",
      "scikit-learn==1.3.2\n",
      "scipy==1.15.3\n",
      "seaborn==0.13.0\n",
      "six==1.17.0\n",
      "sniffio==1.3.1\n",
      "SQLAlchemy==2.0.23\n",
      "stack-data==0.6.3\n",
      "stanio==0.5.1\n",
      "starlette==0.27.0\n",
      "statsmodels==0.14.0\n",
      "tenacity==9.1.2\n",
      "threadpoolctl==3.6.0\n",
      "tomli==2.2.1\n",
      "tornado==6.5.1\n",
      "tqdm==4.67.1\n",
      "traitlets==5.14.3\n",
      "typing_extensions==4.14.1\n",
      "tzdata==2025.2\n",
      "urllib3==2.5.0\n",
      "uvicorn==0.24.0\n",
      "watchfiles==1.1.0\n",
      "wcwidth==0.2.13\n",
      "websockets==15.0.1\n",
      "xgboost==2.0.1\n",
      "\n",
      "Missing packages: {'uvicorn[standard]', 'babel', '# Retail Intelligence Platform Dependencies', 'jinja2', 'sqlalchemy'}\n"
     ]
    }
   ],
   "source": [
    "# Check All Dependencies Installed Without Errors\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# List installed packages\n",
    "installed = subprocess.check_output([sys.executable, '-m', 'pip', 'freeze']).decode()\n",
    "print('Installed packages:')\n",
    "print(installed)\n",
    "\n",
    "# Compare with requirements.txt\n",
    "with open('requirements.txt', encoding='utf-8') as f:\n",
    "    required = set(line.strip().split('==')[0] for line in f if line.strip() and not line.startswith('#'))\n",
    "installed_pkgs = set(line.split('==')[0] for line in installed.splitlines())\n",
    "missing = required - installed_pkgs\n",
    "if missing:\n",
    "    print(f\"Missing packages: {missing}\")\n",
    "else:\n",
    "    print(\"All required packages are installed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a46d2950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.10.9, pytest-7.4.3, pluggy-1.6.0\n",
      "rootdir: d:\\retail-intelligence\n",
      "plugins: anyio-3.7.1, asyncio-0.21.1\n",
      "asyncio: mode=strict\n",
      "collected 5 items\n",
      "\n",
      "test_platform.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[33m                                                   [100%]\u001b[0m\n",
      "\n",
      "\u001b[33m============================== warnings summary ===============================\u001b[0m\n",
      "test_platform.py::test_api_server\n",
      "  d:\\retail-intelligence\\retail_env\\lib\\site-packages\\_pytest\\python.py:198: PytestReturnNotNoneWarning: Expected None, but test_platform.py::test_api_server returned False, which will be an error in a future version of pytest.  Did you mean to use `assert` instead of `return`?\n",
      "    warnings.warn(\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[33m======================== \u001b[32m5 passed\u001b[0m, \u001b[33m\u001b[1m1 warning\u001b[0m\u001b[33m in 5.08s\u001b[0m\u001b[33m =========================\u001b[0m\n",
      "\n",
      "All tests passed.\n"
     ]
    }
   ],
   "source": [
    "# Run All Tests in test_platform.py\n",
    "import subprocess\n",
    "result = subprocess.run([sys.executable, '-m', 'pytest', 'test_platform.py'], capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "if result.returncode == 0:\n",
    "    print('All tests passed.')\n",
    "else:\n",
    "    print('Some tests failed. See output above.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70aa91da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server running: True\n",
      "Response: {'status': 'healthy', 'app_name': 'Retail Intelligence Platform', 'version': '1.0.0', 'environment': 'development'}\n"
     ]
    }
   ],
   "source": [
    "# Start Server and Verify Running at http://127.0.0.1:8000\n",
    "import requests\n",
    "try:\n",
    "    response = requests.get('http://127.0.0.1:8000/health', timeout=5)\n",
    "    print('Server running:', response.status_code == 200)\n",
    "    print('Response:', response.json())\n",
    "except Exception as e:\n",
    "    print('Server not running or not accessible:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6b4bced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API docs accessible: True\n",
      "Docs HTML preview: \n",
      "    <!DOCTYPE html>\n",
      "    <html>\n",
      "    <head>\n",
      "    <link type=\"text/css\" rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/swagger-ui-dist@5.9.0/swagger-ui.css\">\n",
      "    <link rel=\"shortcut icon\" href=\"https://fastapi.tiangolo.com/img/favicon.png\">\n",
      "    <title>Retail Intelligence Platform - Swagger UI</title>\n",
      "    </head>\n",
      "    <body>\n",
      "    <div id=\"swagger-ui\">\n",
      "    </div>\n",
      "    <script src=\"https://cdn.jsdelivr.net/npm/swagger-ui-dist@5.9.0/swagger-ui-bundle.js\"></script>\n",
      "    <!-- `SwaggerUIBundle` is now av\n"
     ]
    }
   ],
   "source": [
    "# Access API Documentation at http://127.0.0.1:8000/docs\n",
    "try:\n",
    "    response = requests.get('http://127.0.0.1:8000/docs', timeout=5)\n",
    "    print('API docs accessible:', response.status_code == 200)\n",
    "    print('Docs HTML preview:', response.text[:500])\n",
    "except Exception as e:\n",
    "    print('API docs not accessible:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f406e3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Sample data generated successfully!\n",
      "ðŸ“„ File: data/samples/indian_retail_sample.csv\n",
      "ðŸ“Š Records: 4,177\n",
      "ðŸ“… Period: 2025-02-01 to 2025-07-31\n",
      "ðŸ’° Total Sales: â‚¹527,841.64\n",
      "ðŸ›ï¸ Products: 20\n",
      "ðŸ‘¥ Customers: 300\n",
      "ðŸ’³ Avg Transaction: â‚¹126.37\n",
      "Generated 4177 records\n",
      "Date range: 2025-02-01 to 2025-07-31\n",
      "Total sales: â‚¹527,841.64\n"
     ]
    }
   ],
   "source": [
    "# Generate Sample Data and Verify Success\n",
    "try:\n",
    "    from generate_sample_data import generate_sample_retail_data\n",
    "    df, summary = generate_sample_retail_data()\n",
    "    print(f\"Generated {summary['total_records']} records\")\n",
    "    print(f\"Date range: {summary['date_range']}\")\n",
    "    print(f\"Total sales: â‚¹{summary['total_sales']:,.2f}\")\n",
    "except Exception as e:\n",
    "    print('Sample data generation failed:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e127c856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload response: 404\n",
      "Response JSON: {'error': 'Endpoint not found', 'status_code': 404}\n"
     ]
    }
   ],
   "source": [
    "# Test File Upload Functionality\n",
    "import os\n",
    "try:\n",
    "    sample_file = 'data/samples/retail_sample.csv'\n",
    "    url = 'http://127.0.0.1:8000/upload/file'\n",
    "    with open(sample_file, 'rb') as f:\n",
    "        files = {'file': (os.path.basename(sample_file), f, 'text/csv')}\n",
    "        response = requests.post(url, files=files)\n",
    "    print('Upload response:', response.status_code)\n",
    "    print('Response JSON:', response.json())\n",
    "except Exception as e:\n",
    "    print('File upload failed:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5a24c535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking http://127.0.0.1:8000/api/v1/upload/list ...\n",
      "Status: 200\n",
      "Response: {'success': True, 'files': [{'file_id': '26afd863-745b-4151-8854-c4bb266b9636', 'filename': 'retail_sample.csv', 'status': 'completed', 'uploaded_at': '2025-07-31T12:27:46.979281', 'completed_at': '2025-07-31T12:27:47.006245', 'file_size': 710}, {'file_id': '99696596-c613-4992-8299-bdac25475181', 'filename': 'retail_sample.csv', 'status': 'completed', 'uploaded_at': '2025-07-31T12:26:25.686658', 'completed_at': '2025-07-31T12:26:25.725872', 'file_size': 710}, {'file_id': '47d3cab4-33d2-4c47-87df-b2d7603c9ffa', 'filename': 'retail_sample.csv', 'status': 'completed', 'uploaded_at': '2025-07-31T12:26:01.788723', 'completed_at': '2025-07-31T12:26:01.807646', 'file_size': 710}, {'file_id': '4bbf39b9-a48f-4865-9144-28cb30516a83', 'filename': 'retail_sample.csv', 'status': 'completed', 'uploaded_at': '2025-07-31T12:23:11.958354', 'completed_at': '2025-07-31T12:23:11.986129', 'file_size': 710}], 'total_files': 4}\n",
      "Checking http://127.0.0.1:8000/api/v1/predict ...\n",
      "Status: 500\n",
      "Response: {'detail': ''}\n",
      "Checking http://127.0.0.1:8000/api/v1/models ...\n",
      "Status: 200\n",
      "Response: {'success': True, 'models': {'prophet': {'name': 'Prophet', 'description': \"Facebook's time series forecasting tool, excellent for seasonal data\", 'best_for': ['Strong seasonal patterns', 'Holiday effects', 'Missing data tolerance', 'Retail sales forecasting'], 'requirements': {'min_data_points': 60, 'seasonality': 'preferred', 'missing_data_tolerance': 'high'}, 'outputs': ['point_forecast', 'confidence_intervals', 'trend_components']}, 'xgboost': {'name': 'XGBoost', 'description': 'Gradient boosting for complex pattern recognition with multiple features', 'best_for': ['Multiple input features', 'Non-linear patterns', 'Feature importance analysis', 'Complex business rules'], 'requirements': {'min_data_points': 100, 'features': 'multiple_preferred', 'missing_data_tolerance': 'low'}, 'outputs': ['point_forecast', 'feature_importance', 'confidence_intervals']}, 'arima': {'name': 'ARIMA', 'description': 'Classical time series model for stationary data with autocorrelation', 'best_for': ['Stationary time series', 'Simple trend patterns', 'Quick forecasting', 'Traditional time series analysis'], 'requirements': {'min_data_points': 30, 'stationarity': 'preferred', 'missing_data_tolerance': 'very_low'}, 'outputs': ['point_forecast', 'confidence_intervals', 'model_diagnostics']}}, 'recommendation': 'Use auto-forecast endpoint for automatic model selection', 'selection_criteria': {'data_characteristics': 'Seasonality, trend, stationarity', 'data_quality': 'Completeness, consistency', 'feature_availability': 'Additional variables for XGBoost', 'business_context': 'Indian retail patterns and holidays'}}\n",
      "Checking http://127.0.0.1:8000/api/v1/models/compare/sample ...\n",
      "Status: 404\n",
      "Response: {'error': 'Endpoint not found', 'status_code': 404}\n"
     ]
    }
   ],
   "source": [
    "# Verify All API Endpoints Respond Correctly\n",
    "endpoints = [\n",
    "    ('/api/v1/upload/list', 'get'),\n",
    "    ('/api/v1/predict', 'post', {'file_id': 'sample', 'model_type': 'prophet', 'forecast_periods': 7}),\n",
    "    ('/api/v1/models', 'get'),\n",
    "    ('/api/v1/models/compare/sample', 'get'),\n",
    "]\n",
    "base_url = 'http://127.0.0.1:8000'\n",
    "for ep in endpoints:\n",
    "    url = base_url + ep[0]\n",
    "    method = ep[1]\n",
    "    print(f'Checking {url} ...')\n",
    "    try:\n",
    "        if method == 'get':\n",
    "            response = requests.get(url, timeout=10)\n",
    "        elif method == 'post':\n",
    "            response = requests.post(url, json=ep[2], timeout=10)\n",
    "        print('Status:', response.status_code)\n",
    "        print('Response:', response.json() if 'application/json' in response.headers.get('content-type','') else response.text[:500])\n",
    "    except Exception as e:\n",
    "        print(f'Error accessing {url}:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "279e8578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload response: 202\n",
      "Response JSON: {'success': True, 'data': {'file_id': '8596be8c-6345-4523-b84a-754ba5705749', 'filename': 'retail_sample.csv', 'status': 'processing'}, 'message': 'File uploaded successfully. Processing started.'}\n",
      "Could not extract file_id from response.\n"
     ]
    }
   ],
   "source": [
    "# Upload a sample file and get file_id for forecasting\n",
    "import os\n",
    "sample_file = 'data/samples/retail_sample.csv'\n",
    "url = 'http://127.0.0.1:8000/api/v1/upload'\n",
    "file_id = None\n",
    "try:\n",
    "    with open(sample_file, 'rb') as f:\n",
    "        files = {'file': (os.path.basename(sample_file), f, 'text/csv')}\n",
    "        response = requests.post(url, files=files)\n",
    "    print('Upload response:', response.status_code)\n",
    "    print('Response JSON:', response.json())\n",
    "    if response.status_code == 200 and 'file_id' in response.json():\n",
    "        file_id = response.json()['file_id']\n",
    "    else:\n",
    "        print('Could not extract file_id from response.')\n",
    "except Exception as e:\n",
    "    print('File upload failed:', e)\n",
    "file_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01e8f220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check 1: {'success': True, 'file_id': '8596be8c-6345-4523-b84a-754ba5705749', 'status': 'completed', 'progress': 100, 'results': {'success': True, 'data': 'Saved to processed_8596be8c-6345-4523-b84a-754ba5705749.csv', 'original_shape': [20, 5], 'processed_shape': [20, 8], 'column_analysis': {'date_column': 'date', 'sales_column': 'sales', 'quantity_column': 'quantity', 'product_column': 'product', 'customer_column': 'customer', 'numeric_columns': ['sales', 'quantity'], 'categorical_columns': ['date', 'product', 'customer'], 'detected_patterns': {'date': 'date', 'product': 'product', 'sales': 'sales', 'quantity': 'quantity', 'customer': 'customer'}}, 'validation_results': {'has_date_column': True, 'has_sales_column': True, 'has_sufficient_data': False, 'date_range_days': 9, 'missing_data_percentage': 0, 'seasonal_potential': False, 'trend_potential': True, 'issues': ['Less than 3 months of historical data', 'Insufficient data points: 20 (minimum 30 required)']}, 'quality_score': 68.0, 'summary_stats': {'total_records': 20, 'date_range': {'start_date': '2025-06-01', 'end_date': '2025-06-10', 'total_days': 9, 'data_points': 20}, 'sales_summary': {'total_sales': 'Rs.2.29K', 'average_transaction': 'Rs.114.50', 'median_transaction': 'Rs.107.50', 'max_transaction': 'Rs.165.00', 'min_transaction': 'Rs.80.00', 'std_deviation': 'Rs.27.24'}, 'top_products': [{'product': 'Shampoo', 'total_sales': 'Rs.1.15K', 'transaction_count': 8}, {'product': 'Toothpaste', 'total_sales': 'Rs.615.00', 'transaction_count': 6}, {'product': 'Soap', 'total_sales': 'Rs.530.00', 'transaction_count': 6}], 'time_patterns': {'best_day_of_week': 'Saturday', 'worst_day_of_week': 'Thursday', 'best_month': 'June', 'worst_month': 'June', 'weekend_vs_weekday': {'weekend_average': 'Rs.110.33', 'weekday_average': 'Rs.116.29', 'weekend_lift_percentage': -5.12}}}, 'recommendations': ['ðŸ“… Collect more historical data (minimum 3 months) for better forecasting accuracy.', 'ðŸ”„ Limited seasonal analysis possible. Collect data over multiple seasons.', 'ðŸ”§ Address data structure issues before proceeding with forecasting.', 'âœ… Good data quality. Suitable for reliable forecasting.'], 'processed_file_path': 'D:\\\\retail-intelligence\\\\data\\\\processed\\\\processed_8596be8c-6345-4523-b84a-754ba5705749.csv'}, 'error': None}\n",
      "File processing completed.\n"
     ]
    }
   ],
   "source": [
    "# Extract file_id from upload response and poll for completion\n",
    "import time\n",
    "file_id = '8596be8c-6345-4523-b84a-754ba5705749'  # Extracted from previous response\n",
    "status_url = f'http://127.0.0.1:8000/api/v1/upload/status/{file_id}'\n",
    "for i in range(10):\n",
    "    response = requests.get(status_url)\n",
    "    print(f'Check {i+1}:', response.json())\n",
    "    status = response.json().get('status')\n",
    "    if status == 'completed':\n",
    "        print('File processing completed.')\n",
    "        break\n",
    "    time.sleep(2)\n",
    "else:\n",
    "    print('File processing not completed after polling.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "faacbded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file_id: 26afd863-745b-4151-8854-c4bb266b9636\n",
      "Processing status: completed\n",
      "Training job_id: 9a145f12-c1d8-45b7-9144-08b9d3a13247\n",
      "Training status: completed\n",
      "Checking http://127.0.0.1:8000/api/v1/upload/list ...\n",
      "Status: 200\n",
      "Response: {'success': True, 'files': [{'file_id': '26afd863-745b-4151-8854-c4bb266b9636', 'filename': 'retail_sample.csv', 'status': 'completed', 'uploaded_at': '2025-07-31T12:27:46.979281', 'completed_at': '2025-07-31T12:27:47.006245', 'file_size': 710}, {'file_id': '99696596-c613-4992-8299-bdac25475181', 'filename': 'retail_sample.csv', 'status': 'completed', 'uploaded_at': '2025-07-31T12:26:25.686658', 'completed_at': '2025-07-31T12:26:25.725872', 'file_size': 710}, {'file_id': '47d3cab4-33d2-4c47-87df-b2d7603c9ffa', 'filename': 'retail_sample.csv', 'status': 'completed', 'uploaded_at': '2025-07-31T12:26:01.788723', 'completed_at': '2025-07-31T12:26:01.807646', 'file_size': 710}, {'file_id': '4bbf39b9-a48f-4865-9144-28cb30516a83', 'filename': 'retail_sample.csv', 'status': 'completed', 'uploaded_at': '2025-07-31T12:23:11.958354', 'completed_at': '2025-07-31T12:23:11.986129', 'file_size': 710}], 'total_files': 4}\n",
      "Checking http://127.0.0.1:8000/api/v1/predict ...\n",
      "Status: 200\n",
      "Response: {'success': True, 'forecast': [{'ds': '2025-06-11', 'yhat': 273.9526875833591, 'yhat_lower': 273.952618667552, 'yhat_upper': 273.95275712767534}, {'ds': '2025-06-12', 'yhat': 83.22731900187956, 'yhat_lower': 83.22725543324638, 'yhat_upper': 83.22738397873425}, {'ds': '2025-06-13', 'yhat': -20.186496013788748, 'yhat_lower': -20.18652472883213, 'yhat_upper': -20.18646660504675}, {'ds': '2025-06-14', 'yhat': -120.17341471865208, 'yhat_lower': -120.17368593480387, 'yhat_upper': -120.17312894439016}, {'ds': '2025-06-15', 'yhat': -241.61003797956243, 'yhat_lower': -241.61080986381904, 'yhat_upper': -241.60923584137714}, {'ds': '2025-06-16', 'yhat': -181.934412975195, 'yhat_lower': -181.93516481659734, 'yhat_upper': -181.9336191824837}, {'ds': '2025-06-17', 'yhat': -143.23672605415862, 'yhat_lower': -143.23748329068857, 'yhat_upper': -143.2359527005984}], 'model_used': 'prophet', 'model_id': 'prophet_20250731_122759', 'selection_confidence': 1.0, 'training_metrics': {'mae': 0.00014848814977597158, 'rmse': 0.0001866013510025457, 'mape': 6.840243043211996e-05, 'r2': 0.9999999999569484}, 'forecast_periods': 7, 'frequency': 'daily', 'generated_at': '2025-07-31T12:27:59.782076', 'model_performance': {'mae': 0.00014848814977597158, 'rmse': 0.0001866013510025457, 'r2': 0.9999999999569484}}\n",
      "Checking http://127.0.0.1:8000/api/v1/models ...\n",
      "Status: 200\n",
      "Response: {'success': True, 'models': {'prophet': {'name': 'Prophet', 'description': \"Facebook's time series forecasting tool, excellent for seasonal data\", 'best_for': ['Strong seasonal patterns', 'Holiday effects', 'Missing data tolerance', 'Retail sales forecasting'], 'requirements': {'min_data_points': 60, 'seasonality': 'preferred', 'missing_data_tolerance': 'high'}, 'outputs': ['point_forecast', 'confidence_intervals', 'trend_components']}, 'xgboost': {'name': 'XGBoost', 'description': 'Gradient boosting for complex pattern recognition with multiple features', 'best_for': ['Multiple input features', 'Non-linear patterns', 'Feature importance analysis', 'Complex business rules'], 'requirements': {'min_data_points': 100, 'features': 'multiple_preferred', 'missing_data_tolerance': 'low'}, 'outputs': ['point_forecast', 'feature_importance', 'confidence_intervals']}, 'arima': {'name': 'ARIMA', 'description': 'Classical time series model for stationary data with autocorrelation', 'best_for': ['Stationary time series', 'Simple trend patterns', 'Quick forecasting', 'Traditional time series analysis'], 'requirements': {'min_data_points': 30, 'stationarity': 'preferred', 'missing_data_tolerance': 'very_low'}, 'outputs': ['point_forecast', 'confidence_intervals', 'model_diagnostics']}}, 'recommendation': 'Use auto-forecast endpoint for automatic model selection', 'selection_criteria': {'data_characteristics': 'Seasonality, trend, stationarity', 'data_quality': 'Completeness, consistency', 'feature_availability': 'Additional variables for XGBoost', 'business_context': 'Indian retail patterns and holidays'}}\n",
      "Checking http://127.0.0.1:8000/api/v1/models/compare/26afd863-745b-4151-8854-c4bb266b9636 ...\n",
      "Training status: completed\n",
      "Checking http://127.0.0.1:8000/api/v1/upload/list ...\n",
      "Status: 200\n",
      "Response: {'success': True, 'files': [{'file_id': '26afd863-745b-4151-8854-c4bb266b9636', 'filename': 'retail_sample.csv', 'status': 'completed', 'uploaded_at': '2025-07-31T12:27:46.979281', 'completed_at': '2025-07-31T12:27:47.006245', 'file_size': 710}, {'file_id': '99696596-c613-4992-8299-bdac25475181', 'filename': 'retail_sample.csv', 'status': 'completed', 'uploaded_at': '2025-07-31T12:26:25.686658', 'completed_at': '2025-07-31T12:26:25.725872', 'file_size': 710}, {'file_id': '47d3cab4-33d2-4c47-87df-b2d7603c9ffa', 'filename': 'retail_sample.csv', 'status': 'completed', 'uploaded_at': '2025-07-31T12:26:01.788723', 'completed_at': '2025-07-31T12:26:01.807646', 'file_size': 710}, {'file_id': '4bbf39b9-a48f-4865-9144-28cb30516a83', 'filename': 'retail_sample.csv', 'status': 'completed', 'uploaded_at': '2025-07-31T12:23:11.958354', 'completed_at': '2025-07-31T12:23:11.986129', 'file_size': 710}], 'total_files': 4}\n",
      "Checking http://127.0.0.1:8000/api/v1/predict ...\n",
      "Status: 200\n",
      "Response: {'success': True, 'forecast': [{'ds': '2025-06-11', 'yhat': 273.9526875833591, 'yhat_lower': 273.952618667552, 'yhat_upper': 273.95275712767534}, {'ds': '2025-06-12', 'yhat': 83.22731900187956, 'yhat_lower': 83.22725543324638, 'yhat_upper': 83.22738397873425}, {'ds': '2025-06-13', 'yhat': -20.186496013788748, 'yhat_lower': -20.18652472883213, 'yhat_upper': -20.18646660504675}, {'ds': '2025-06-14', 'yhat': -120.17341471865208, 'yhat_lower': -120.17368593480387, 'yhat_upper': -120.17312894439016}, {'ds': '2025-06-15', 'yhat': -241.61003797956243, 'yhat_lower': -241.61080986381904, 'yhat_upper': -241.60923584137714}, {'ds': '2025-06-16', 'yhat': -181.934412975195, 'yhat_lower': -181.93516481659734, 'yhat_upper': -181.9336191824837}, {'ds': '2025-06-17', 'yhat': -143.23672605415862, 'yhat_lower': -143.23748329068857, 'yhat_upper': -143.2359527005984}], 'model_used': 'prophet', 'model_id': 'prophet_20250731_122759', 'selection_confidence': 1.0, 'training_metrics': {'mae': 0.00014848814977597158, 'rmse': 0.0001866013510025457, 'mape': 6.840243043211996e-05, 'r2': 0.9999999999569484}, 'forecast_periods': 7, 'frequency': 'daily', 'generated_at': '2025-07-31T12:27:59.782076', 'model_performance': {'mae': 0.00014848814977597158, 'rmse': 0.0001866013510025457, 'r2': 0.9999999999569484}}\n",
      "Checking http://127.0.0.1:8000/api/v1/models ...\n",
      "Status: 200\n",
      "Response: {'success': True, 'models': {'prophet': {'name': 'Prophet', 'description': \"Facebook's time series forecasting tool, excellent for seasonal data\", 'best_for': ['Strong seasonal patterns', 'Holiday effects', 'Missing data tolerance', 'Retail sales forecasting'], 'requirements': {'min_data_points': 60, 'seasonality': 'preferred', 'missing_data_tolerance': 'high'}, 'outputs': ['point_forecast', 'confidence_intervals', 'trend_components']}, 'xgboost': {'name': 'XGBoost', 'description': 'Gradient boosting for complex pattern recognition with multiple features', 'best_for': ['Multiple input features', 'Non-linear patterns', 'Feature importance analysis', 'Complex business rules'], 'requirements': {'min_data_points': 100, 'features': 'multiple_preferred', 'missing_data_tolerance': 'low'}, 'outputs': ['point_forecast', 'feature_importance', 'confidence_intervals']}, 'arima': {'name': 'ARIMA', 'description': 'Classical time series model for stationary data with autocorrelation', 'best_for': ['Stationary time series', 'Simple trend patterns', 'Quick forecasting', 'Traditional time series analysis'], 'requirements': {'min_data_points': 30, 'stationarity': 'preferred', 'missing_data_tolerance': 'very_low'}, 'outputs': ['point_forecast', 'confidence_intervals', 'model_diagnostics']}}, 'recommendation': 'Use auto-forecast endpoint for automatic model selection', 'selection_criteria': {'data_characteristics': 'Seasonality, trend, stationarity', 'data_quality': 'Completeness, consistency', 'feature_availability': 'Additional variables for XGBoost', 'business_context': 'Indian retail patterns and holidays'}}\n",
      "Checking http://127.0.0.1:8000/api/v1/models/compare/26afd863-745b-4151-8854-c4bb266b9636 ...\n",
      "Error accessing http://127.0.0.1:8000/api/v1/models/compare/26afd863-745b-4151-8854-c4bb266b9636: HTTPConnectionPool(host='127.0.0.1', port=8000): Read timed out. (read timeout=10)\n",
      "Error accessing http://127.0.0.1:8000/api/v1/models/compare/26afd863-745b-4151-8854-c4bb266b9636: HTTPConnectionPool(host='127.0.0.1', port=8000): Read timed out. (read timeout=10)\n"
     ]
    }
   ],
   "source": [
    "# Verify All API Endpoints Respond Correctly with Real Data\n",
    "import requests, time\n",
    "base_url = 'http://127.0.0.1:8000'\n",
    "sample_file = 'data/samples/retail_sample.csv'\n",
    "files = {'file': open(sample_file, 'rb')}\n",
    "\n",
    "# Upload file\n",
    "response = requests.post(f'{base_url}/api/v1/upload', files=files)\n",
    "try:\n",
    "    resp_json = response.json()\n",
    "except Exception:\n",
    "    print('Upload failed. Response:', response.text)\n",
    "    raise\n",
    "file_id = resp_json.get('file_id') or (resp_json.get('data', {}) or {}).get('file_id')\n",
    "print('Uploaded file_id:', file_id)\n",
    "if not file_id:\n",
    "    print('Upload failed. Response:', resp_json)\n",
    "    raise Exception('Upload failed, no file_id returned')\n",
    "\n",
    "# Poll for processing completion\n",
    "list_url = f'{base_url}/api/v1/upload/list'\n",
    "status = None\n",
    "for _ in range(30):\n",
    "    response = requests.get(list_url)\n",
    "    result = response.json()\n",
    "    status = next((f['status'] for f in result['files'] if f['file_id'] == file_id), None)\n",
    "    print('Processing status:', status)\n",
    "    if status == 'completed':\n",
    "        break\n",
    "    time.sleep(2)\n",
    "if status != 'completed':\n",
    "    print('File processing did not complete.')\n",
    "    raise Exception('Processing failed')\n",
    "\n",
    "# Train model (Prophet)\n",
    "train_url = f'{base_url}/api/v1/train'\n",
    "train_req = {'file_id': file_id, 'model_type': 'prophet'}\n",
    "response = requests.post(train_url, json=train_req)\n",
    "train_json = response.json()\n",
    "job_id = train_json.get('job_id')\n",
    "print('Training job_id:', job_id)\n",
    "\n",
    "# Poll for training completion\n",
    "status_url = f'{base_url}/api/v1/train/status/{job_id}'\n",
    "train_status = None\n",
    "for _ in range(30):\n",
    "    response = requests.get(status_url)\n",
    "    train_status = response.json()\n",
    "    print('Training status:', train_status.get('status'))\n",
    "    if train_status.get('status') == 'completed':\n",
    "        break\n",
    "    if train_status.get('status') == 'failed':\n",
    "        print('Training failed:', train_status.get('error'))\n",
    "        raise Exception('Training failed')\n",
    "    time.sleep(2)\n",
    "if train_status.get('status') != 'completed':\n",
    "    print('Model training did not complete.')\n",
    "    raise Exception('Training failed')\n",
    "\n",
    "# Endpoints to verify\n",
    "endpoints = [\n",
    "    ('/api/v1/upload/list', 'get'),\n",
    "    ('/api/v1/predict', 'post', {'file_id': file_id, 'model_type': 'prophet', 'forecast_periods': 7}),\n",
    "    ('/api/v1/models', 'get'),\n",
    "    (f'/api/v1/models/compare/{file_id}', 'get'),\n",
    "]\n",
    "for ep in endpoints:\n",
    "    url = base_url + ep[0]\n",
    "    method = ep[1]\n",
    "    print(f'Checking {url} ...')\n",
    "    try:\n",
    "        if method == 'get':\n",
    "            response = requests.get(url, timeout=10)\n",
    "        elif method == 'post':\n",
    "            response = requests.post(url, json=ep[2], timeout=10)\n",
    "        print('Status:', response.status_code)\n",
    "        print('Response:', response.json() if 'application/json' in response.headers.get('content-type','') else response.text[:500])\n",
    "    except Exception as e:\n",
    "        print(f'Error accessing {url}:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3063e711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No uploaded files found.\n"
     ]
    }
   ],
   "source": [
    "# Get a valid file_id from uploaded files and use for forecast endpoints\n",
    "list_url = 'http://127.0.0.1:8000/api/v1/upload/list'\n",
    "response = requests.get(list_url)\n",
    "files = response.json().get('files', [])\n",
    "file_id = None\n",
    "if files:\n",
    "    file_id = files[0]['file_id']\n",
    "    print(f'Using file_id: {file_id}')\n",
    "else:\n",
    "    print('No uploaded files found.')\n",
    "\n",
    "if file_id:\n",
    "    endpoints = [\n",
    "        ('/api/v1/predict', 'post', {'file_id': file_id, 'model_type': 'prophet', 'forecast_periods': 7}),\n",
    "        (f'/api/v1/models/compare/{file_id}', 'get'),\n",
    "    ]\n",
    "    base_url = 'http://127.0.0.1:8000'\n",
    "    for ep in endpoints:\n",
    "        url = base_url + ep[0]\n",
    "        method = ep[1]\n",
    "        print(f'Checking {url} ...')\n",
    "        try:\n",
    "            if method == 'get':\n",
    "                response = requests.get(url, timeout=30)\n",
    "            elif method == 'post':\n",
    "                response = requests.post(url, json=ep[2], timeout=30)\n",
    "            print('Status:', response.status_code)\n",
    "            print('Response:', response.json() if 'application/json' in response.headers.get('content-type','') else response.text[:500])\n",
    "        except Exception as e:\n",
    "            print(f'Error accessing {url}:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "594319d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload response: 202\n",
      "Uploaded file_id: 72b3ea82-f8bb-4fb9-9dab-b5be76da029f\n",
      "Check 1: completed\n",
      "File processing completed.\n",
      "Checking http://127.0.0.1:8000/api/v1/predict ...\n",
      "Error accessing http://127.0.0.1:8000/api/v1/predict: HTTPConnectionPool(host='127.0.0.1', port=8000): Read timed out. (read timeout=30)\n",
      "Checking http://127.0.0.1:8000/api/v1/models/compare/72b3ea82-f8bb-4fb9-9dab-b5be76da029f ...\n",
      "Error accessing http://127.0.0.1:8000/api/v1/predict: HTTPConnectionPool(host='127.0.0.1', port=8000): Read timed out. (read timeout=30)\n",
      "Checking http://127.0.0.1:8000/api/v1/models/compare/72b3ea82-f8bb-4fb9-9dab-b5be76da029f ...\n",
      "Error accessing http://127.0.0.1:8000/api/v1/models/compare/72b3ea82-f8bb-4fb9-9dab-b5be76da029f: HTTPConnectionPool(host='127.0.0.1', port=8000): Read timed out. (read timeout=30)\n",
      "Error accessing http://127.0.0.1:8000/api/v1/models/compare/72b3ea82-f8bb-4fb9-9dab-b5be76da029f: HTTPConnectionPool(host='127.0.0.1', port=8000): Read timed out. (read timeout=30)\n"
     ]
    }
   ],
   "source": [
    "# Full workflow: upload, poll, train, poll, forecast\n",
    "import requests, time\n",
    "base_url = \"http://localhost:8000/api/v1\"\n",
    "sample_file = \"data/samples/retail_sample.csv\"\n",
    "files = {\"file\": open(sample_file, \"rb\")}\n",
    "\n",
    "# Upload file\n",
    "response = requests.post(f\"{base_url}/upload\", files=files)\n",
    "resp_json = response.json()\n",
    "file_id = resp_json.get(\"file_id\")\n",
    "print(\"Uploaded file_id:\", file_id)\n",
    "\n",
    "# Poll for processing completion\n",
    "list_url = f\"{base_url}/upload/list\"\n",
    "status = None\n",
    "for _ in range(30):\n",
    "    response = requests.get(list_url)\n",
    "    result = response.json()\n",
    "    status = next((f[\"status\"] for f in result[\"files\"] if f[\"file_id\"] == file_id), None)\n",
    "    print(\"Processing status:\", status)\n",
    "    if status == \"completed\":\n",
    "        break\n",
    "    time.sleep(2)\n",
    "if status != \"completed\":\n",
    "    print(\"File processing did not complete.\")\n",
    "    raise Exception(\"Processing failed\")\n",
    "\n",
    "# Train model (Prophet)\n",
    "train_url = f\"{base_url}/train\"\n",
    "train_req = {\"file_id\": file_id, \"model_type\": \"prophet\"}\n",
    "response = requests.post(train_url, json=train_req)\n",
    "train_json = response.json()\n",
    "job_id = train_json.get(\"job_id\")\n",
    "print(\"Training job_id:\", job_id)\n",
    "\n",
    "# Poll for training completion\n",
    "status_url = f\"{base_url}/train/status/{job_id}\"\n",
    "train_status = None\n",
    "for _ in range(30):\n",
    "    response = requests.get(status_url)\n",
    "    train_status = response.json()\n",
    "    print(\"Training status:\", train_status.get(\"status\"))\n",
    "    if train_status.get(\"status\") == \"completed\":\n",
    "        break\n",
    "    if train_status.get(\"status\") == \"failed\":\n",
    "        print(\"Training failed:\", train_status.get(\"error\"))\n",
    "        raise Exception(\"Training failed\")\n",
    "    time.sleep(2)\n",
    "if train_status.get(\"status\") != \"completed\":\n",
    "    print(\"Model training did not complete.\")\n",
    "    raise Exception(\"Training failed\")\n",
    "\n",
    "# Forecast\n",
    "predict_url = f\"{base_url}/predict\"\n",
    "predict_req = {\"file_id\": file_id, \"model_type\": \"prophet\", \"forecast_periods\": 30}\n",
    "response = requests.post(predict_url, json=predict_req)\n",
    "predict_json = response.json()\n",
    "if not predict_json.get(\"success\"):\n",
    "    print(\"Forecast error:\", predict_json.get(\"message\", predict_json.get(\"error\")))\n",
    "else:\n",
    "    print(\"Forecast result:\")\n",
    "    print(predict_json[\"forecast\"][:5])  # Show first 5 forecast rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d8793688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict status: 500\n",
      "Predict response: {'detail': 'Object of type Timestamp is not JSON serializable'}\n",
      "Error accessing /models/compare: HTTPConnectionPool(host='127.0.0.1', port=8000): Read timed out. (read timeout=15)\n",
      "Error accessing /models/compare: HTTPConnectionPool(host='127.0.0.1', port=8000): Read timed out. (read timeout=15)\n"
     ]
    }
   ],
   "source": [
    "# Test /predict and /models/compare endpoints with provided file ID\n",
    "import requests\n",
    "base_url = 'http://127.0.0.1:8000/api/v1'\n",
    "file_id = '48649579-6764-4fcc-939e-e9f3cca93514'\n",
    "\n",
    "# Test /predict\n",
    "predict_url = f'{base_url}/predict'\n",
    "predict_req = {'file_id': file_id, 'model_type': 'prophet', 'forecast_periods': 7}\n",
    "try:\n",
    "    response = requests.post(predict_url, json=predict_req, timeout=15)\n",
    "    print('Predict status:', response.status_code)\n",
    "    if 'application/json' in response.headers.get('content-type',''):\n",
    "        print('Predict response:', response.json())\n",
    "    else:\n",
    "        print('Predict response:', response.text[:500])\n",
    "except Exception as e:\n",
    "    print('Error accessing /predict:', e)\n",
    "\n",
    "# Test /models/compare\n",
    "compare_url = f'{base_url}/models/compare/{file_id}'\n",
    "try:\n",
    "    response = requests.get(compare_url, timeout=15)\n",
    "    print('Compare status:', response.status_code)\n",
    "    if 'application/json' in response.headers.get('content-type',''):\n",
    "        print('Compare response:', response.json())\n",
    "    else:\n",
    "        print('Compare response:', response.text[:500])\n",
    "except Exception as e:\n",
    "    print('Error accessing /models/compare:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5a5cdbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File List Status: 200\n",
      "File List Response: {'success': True, 'files': [], 'total_files': 0}\n",
      "File not found: 48649579-6764-4fcc-939e-e9f3cca93514\n",
      "Predict Status: 500\n",
      "Predict Response: {'detail': ''}\n",
      "Auto-Forecast Status: 422\n",
      "Auto-Forecast Response: {'detail': [{'type': 'missing', 'loc': ['query', 'file_id'], 'msg': 'Field required', 'input': None, 'url': 'https://errors.pydantic.dev/2.5/v/missing'}]}\n",
      "Compare Status: 404\n",
      "Compare Response: {'error': 'Endpoint not found', 'status_code': 404}\n"
     ]
    }
   ],
   "source": [
    "# Practical API Test Client for Forecast Endpoints\n",
    "import requests\n",
    "\n",
    "class ForecastAPITester:\n",
    "    def __init__(self, base_url=\"http://127.0.0.1:8000/api/v1\"):\n",
    "        self.base_url = base_url\n",
    "\n",
    "    def get_file_list(self):\n",
    "        url = f\"{self.base_url}/upload/list\"\n",
    "        resp = requests.get(url)\n",
    "        print(\"File List Status:\", resp.status_code)\n",
    "        print(\"File List Response:\", resp.json())\n",
    "        return resp.json().get(\"files\", [])\n",
    "\n",
    "    def get_file_info(self, file_id):\n",
    "        files = self.get_file_list()\n",
    "        for f in files:\n",
    "            if f[\"file_id\"] == file_id:\n",
    "                print(\"File Info:\", f)\n",
    "                return f\n",
    "        print(\"File not found:\", file_id)\n",
    "        return None\n",
    "\n",
    "    def test_predict(self, file_id, model_type=\"prophet\", forecast_periods=7, frequency=\"daily\"):\n",
    "        url = f\"{self.base_url}/predict\"\n",
    "        payload = {\n",
    "            \"file_id\": file_id,\n",
    "            \"model_type\": model_type,\n",
    "            \"forecast_periods\": forecast_periods,\n",
    "            \"frequency\": frequency\n",
    "        }\n",
    "        resp = requests.post(url, json=payload)\n",
    "        print(\"Predict Status:\", resp.status_code)\n",
    "        try:\n",
    "            print(\"Predict Response:\", resp.json())\n",
    "        except Exception:\n",
    "            print(\"Predict Response (raw):\", resp.text[:500])\n",
    "\n",
    "    def test_auto_forecast(self, file_id, forecast_periods=7):\n",
    "        url = f\"{self.base_url}/auto-forecast\"\n",
    "        payload = {\n",
    "            \"file_id\": file_id,\n",
    "            \"forecast_periods\": forecast_periods\n",
    "        }\n",
    "        resp = requests.post(url, json=payload)\n",
    "        print(\"Auto-Forecast Status:\", resp.status_code)\n",
    "        try:\n",
    "            print(\"Auto-Forecast Response:\", resp.json())\n",
    "        except Exception:\n",
    "            print(\"Auto-Forecast Response (raw):\", resp.text[:500])\n",
    "\n",
    "    def test_compare(self, file_id):\n",
    "        url = f\"{self.base_url}/models/compare/{file_id}\"\n",
    "        resp = requests.get(url)\n",
    "        print(\"Compare Status:\", resp.status_code)\n",
    "        try:\n",
    "            print(\"Compare Response:\", resp.json())\n",
    "        except Exception:\n",
    "            print(\"Compare Response (raw):\", resp.text[:500])\n",
    "\n",
    "# Usage Example:\n",
    "tester = ForecastAPITester()\n",
    "file_id = \"48649579-6764-4fcc-939e-e9f3cca93514\"  # Replace with your actual file_id\n",
    "tester.get_file_info(file_id)\n",
    "tester.test_predict(file_id)\n",
    "tester.test_auto_forecast(file_id)\n",
    "tester.test_compare(file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65b8330b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file_id: 2ac15af2-19f3-44f9-be28-4466a8f4d8b5\n",
      "Processing status: completed\n",
      "Training job_id: f532c5af-7293-49c7-a3dd-e9bf3ca7f7db\n",
      "Training status: completed\n",
      "\n",
      "--- Endpoint Tests ---\n",
      "Predict Status: 200\n",
      "Predict Response: {'success': True, 'forecast': [{'ds': '2025-06-11', 'yhat': 273.9526875833591, 'yhat_lower': 273.9526087608748, 'yhat_upper': 273.952756313504}, {'ds': '2025-06-12', 'yhat': 83.22731900187956, 'yhat_lower': 83.22724537302477, 'yhat_upper': 83.22738485418813}, {'ds': '2025-06-13', 'yhat': -20.186496013788748, 'yhat_lower': -20.186527097745326, 'yhat_upper': -20.186461713600902}, {'ds': '2025-06-14', 'yhat': -120.17341471865208, 'yhat_lower': -120.17369180462909, 'yhat_upper': -120.17310225949133}, {'ds': '2025-06-15', 'yhat': -241.61003797956243, 'yhat_lower': -241.61083911936743, 'yhat_upper': -241.6091697748569}, {'ds': '2025-06-16', 'yhat': -181.934412975195, 'yhat_lower': -181.9351989730114, 'yhat_upper': -181.93360141041822}, {'ds': '2025-06-17', 'yhat': -143.23672605415862, 'yhat_lower': -143.2374868621093, 'yhat_upper': -143.23594947453347}], 'model_used': 'prophet', 'model_id': 'prophet_20250731_113711', 'selection_confidence': 1.0, 'training_metrics': {'mae': 0.00014848814977597158, 'rmse': 0.0001866013510025457, 'mape': 6.840243043211996e-05, 'r2': 0.9999999999569484}, 'forecast_periods': 7, 'frequency': 'daily', 'generated_at': '2025-07-31T11:37:11.388746', 'model_performance': {'mae': 0.00014848814977597158, 'rmse': 0.0001866013510025457, 'r2': 0.9999999999569484}}\n",
      "Auto-Forecast Status: 405\n",
      "Auto-Forecast Response: {'detail': 'Method Not Allowed'}\n",
      "Training status: completed\n",
      "\n",
      "--- Endpoint Tests ---\n",
      "Predict Status: 200\n",
      "Predict Response: {'success': True, 'forecast': [{'ds': '2025-06-11', 'yhat': 273.9526875833591, 'yhat_lower': 273.9526087608748, 'yhat_upper': 273.952756313504}, {'ds': '2025-06-12', 'yhat': 83.22731900187956, 'yhat_lower': 83.22724537302477, 'yhat_upper': 83.22738485418813}, {'ds': '2025-06-13', 'yhat': -20.186496013788748, 'yhat_lower': -20.186527097745326, 'yhat_upper': -20.186461713600902}, {'ds': '2025-06-14', 'yhat': -120.17341471865208, 'yhat_lower': -120.17369180462909, 'yhat_upper': -120.17310225949133}, {'ds': '2025-06-15', 'yhat': -241.61003797956243, 'yhat_lower': -241.61083911936743, 'yhat_upper': -241.6091697748569}, {'ds': '2025-06-16', 'yhat': -181.934412975195, 'yhat_lower': -181.9351989730114, 'yhat_upper': -181.93360141041822}, {'ds': '2025-06-17', 'yhat': -143.23672605415862, 'yhat_lower': -143.2374868621093, 'yhat_upper': -143.23594947453347}], 'model_used': 'prophet', 'model_id': 'prophet_20250731_113711', 'selection_confidence': 1.0, 'training_metrics': {'mae': 0.00014848814977597158, 'rmse': 0.0001866013510025457, 'mape': 6.840243043211996e-05, 'r2': 0.9999999999569484}, 'forecast_periods': 7, 'frequency': 'daily', 'generated_at': '2025-07-31T11:37:11.388746', 'model_performance': {'mae': 0.00014848814977597158, 'rmse': 0.0001866013510025457, 'r2': 0.9999999999569484}}\n",
      "Auto-Forecast Status: 405\n",
      "Auto-Forecast Response: {'detail': 'Method Not Allowed'}\n",
      "Compare Status: 200\n",
      "Compare Response: {'success': True, 'file_id': '2ac15af2-19f3-44f9-be28-4466a8f4d8b5', 'comparison': {'successful_models': [{'model': 'prophet', 'mae': 0.00014848814977597158, 'rmse': 0.0001866013510025457, 'r2': 0.9999999999569484, 'training_time': 'N/A', 'suitable_for': ['Seasonal retail patterns', 'Holiday impact analysis', 'Missing data handling', 'Long-term trend forecasting']}], 'failed_models': [], 'recommendation': {'model': 'prophet', 'mae': 0.00014848814977597158, 'rmse': 0.0001866013510025457, 'r2': 0.9999999999569484, 'training_time': 'N/A', 'suitable_for': ['Seasonal retail patterns', 'Holiday impact analysis', 'Missing data handling', 'Long-term trend forecasting']}}, 'detailed_results': {'prophet': {'success': True, 'model_id': 'prophet_20250731_113723', 'metrics': {'mae': 0.00014848814977597158, 'rmse': 0.0001866013510025457, 'mape': 6.840243043211996e-05, 'r2': 0.9999999999569484}}}, 'model_ranking': ['prophet_20250731_113723'], 'generated_at': '2025-07-31T11:37:23.827701'}\n",
      "Compare Status: 200\n",
      "Compare Response: {'success': True, 'file_id': '2ac15af2-19f3-44f9-be28-4466a8f4d8b5', 'comparison': {'successful_models': [{'model': 'prophet', 'mae': 0.00014848814977597158, 'rmse': 0.0001866013510025457, 'r2': 0.9999999999569484, 'training_time': 'N/A', 'suitable_for': ['Seasonal retail patterns', 'Holiday impact analysis', 'Missing data handling', 'Long-term trend forecasting']}], 'failed_models': [], 'recommendation': {'model': 'prophet', 'mae': 0.00014848814977597158, 'rmse': 0.0001866013510025457, 'r2': 0.9999999999569484, 'training_time': 'N/A', 'suitable_for': ['Seasonal retail patterns', 'Holiday impact analysis', 'Missing data handling', 'Long-term trend forecasting']}}, 'detailed_results': {'prophet': {'success': True, 'model_id': 'prophet_20250731_113723', 'metrics': {'mae': 0.00014848814977597158, 'rmse': 0.0001866013510025457, 'mape': 6.840243043211996e-05, 'r2': 0.9999999999569484}}}, 'model_ranking': ['prophet_20250731_113723'], 'generated_at': '2025-07-31T11:37:23.827701'}\n"
     ]
    }
   ],
   "source": [
    "# Automated workflow: upload, poll, extract file_id, and test all endpoints\n",
    "import requests, time\n",
    "base_url = 'http://127.0.0.1:8000/api/v1'\n",
    "sample_file = 'data/samples/retail_sample.csv'\n",
    "files = {'file': open(sample_file, 'rb')}\n",
    "\n",
    "# Upload file\n",
    "response = requests.post(f'{base_url}/upload', files=files)\n",
    "resp_json = response.json()\n",
    "file_id = resp_json.get('file_id') or (resp_json.get('data', {}) or {}).get('file_id')\n",
    "print('Uploaded file_id:', file_id)\n",
    "if not file_id:\n",
    "    print('Upload failed. Response:', resp_json)\n",
    "    raise Exception('Upload failed, no file_id returned')\n",
    "\n",
    "# Poll for processing completion\n",
    "list_url = f'{base_url}/upload/list'\n",
    "status = None\n",
    "for _ in range(30):\n",
    "    response = requests.get(list_url)\n",
    "    result = response.json()\n",
    "    status = next((f['status'] for f in result['files'] if f['file_id'] == file_id), None)\n",
    "    print('Processing status:', status)\n",
    "    if status == 'completed':\n",
    "        break\n",
    "    time.sleep(2)\n",
    "if status != 'completed':\n",
    "    print('File processing did not complete.')\n",
    "    raise Exception('Processing failed')\n",
    "\n",
    "# Train model (Prophet)\n",
    "train_url = f'{base_url}/train'\n",
    "train_req = {'file_id': file_id, 'model_type': 'prophet'}\n",
    "response = requests.post(train_url, json=train_req)\n",
    "train_json = response.json()\n",
    "job_id = train_json.get('job_id')\n",
    "print('Training job_id:', job_id)\n",
    "\n",
    "# Poll for training completion\n",
    "status_url = f'{base_url}/train/status/{job_id}'\n",
    "train_status = None\n",
    "for _ in range(30):\n",
    "    response = requests.get(status_url)\n",
    "    train_status = response.json()\n",
    "    print('Training status:', train_status.get('status'))\n",
    "    if train_status.get('status') == 'completed':\n",
    "        break\n",
    "    if train_status.get('status') == 'failed':\n",
    "        print('Training failed:', train_status.get('error'))\n",
    "        raise Exception('Training failed')\n",
    "    time.sleep(2)\n",
    "if train_status.get('status') != 'completed':\n",
    "    print('Model training did not complete.')\n",
    "    raise Exception('Training failed')\n",
    "\n",
    "# Test endpoints with valid file_id\n",
    "print('\\n--- Endpoint Tests ---')\n",
    "# /predict\n",
    "predict_url = f'{base_url}/predict'\n",
    "predict_req = {'file_id': file_id, 'model_type': 'prophet', 'forecast_periods': 7}\n",
    "response = requests.post(predict_url, json=predict_req)\n",
    "print('Predict Status:', response.status_code)\n",
    "print('Predict Response:', response.json() if 'application/json' in response.headers.get('content-type','') else response.text[:500])\n",
    "# /auto-forecast\n",
    "auto_url = f'{base_url}/auto-forecast?file_id={file_id}&forecast_periods=7'\n",
    "response = requests.get(auto_url)\n",
    "print('Auto-Forecast Status:', response.status_code)\n",
    "print('Auto-Forecast Response:', response.json() if 'application/json' in response.headers.get('content-type','') else response.text[:500])\n",
    "# /models/compare\n",
    "compare_url = f'{base_url}/models/compare/{file_id}'\n",
    "response = requests.get(compare_url)\n",
    "print('Compare Status:', response.status_code)\n",
    "print('Compare Response:', response.json() if 'application/json' in response.headers.get('content-type','') else response.text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dff89d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test auto-forecast endpoint with GET and POST methods\n",
    "import requests\n",
    "\n",
    "def test_auto_forecast_methods(file_id):\n",
    "    base_url = \"http://127.0.0.1:8000/api/v1\"\n",
    "    url = f\"{base_url}/auto-forecast?file_id={file_id}&forecast_periods=7\"\n",
    "    # Test GET\n",
    "    resp = requests.get(url)\n",
    "    print(\"GET Status:\", resp.status_code)\n",
    "    print(\"GET Response:\", resp.json() if 'application/json' in resp.headers.get('content-type','') else resp.text[:500])\n",
    "    # Test POST\n",
    "    resp = requests.post(url)\n",
    "    print(\"POST Status:\", resp.status_code)\n",
    "    print(\"POST Response:\", resp.json() if 'application/json' in resp.headers.get('content-type','') else resp.text[:500])\n",
    "\n",
    "# Usage: using the provided file_id\n",
    "test_auto_forecast_methods(\"2ac15af2-19f3-44f9-be28-4466a8f4d8b5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7710fcd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest file_id: 2ac15af2-19f3-44f9-be28-4466a8f4d8b5\n",
      "File info: {'file_id': '2ac15af2-19f3-44f9-be28-4466a8f4d8b5', 'filename': 'retail_sample.csv', 'status': 'completed', 'uploaded_at': '2025-07-31T11:36:58.782776', 'completed_at': '2025-07-31T11:36:58.865323', 'file_size': 710}\n"
     ]
    }
   ],
   "source": [
    "# Print the latest file_id from the upload list for endpoint testing\n",
    "import requests\n",
    "base_url = \"http://127.0.0.1:8000/api/v1\"\n",
    "list_url = f\"{base_url}/upload/list\"\n",
    "response = requests.get(list_url)\n",
    "files = response.json().get(\"files\", [])\n",
    "if files:\n",
    "    latest_file = files[0]\n",
    "    print(\"Latest file_id:\", latest_file[\"file_id\"])\n",
    "    print(\"File info:\", latest_file)\n",
    "else:\n",
    "    print(\"No files found. Please upload a file first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfab0132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Python dependencies...\n",
      "Dependencies: OK\n",
      "\n",
      "Running tests...\n",
      "Tests: \n",
      "\n",
      "Checking server status...\n",
      "Server: OK\n",
      "\n",
      "Checking API docs...\n",
      "API Docs: OK\n",
      "\n",
      "Checking sample data generation...\n",
      "Dependencies: OK\n",
      "\n",
      "Running tests...\n",
      "Tests: \n",
      "\n",
      "Checking server status...\n",
      "Server: OK\n",
      "\n",
      "Checking API docs...\n",
      "API Docs: OK\n",
      "\n",
      "Checking sample data generation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-25 (_readerthread):\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"d:\\retail-intelligence\\retail_env\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 772, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\subprocess.py\", line 1499, in _readerthread\n",
      "    buffer.append(fh.read())\n",
      "  File \"C:\\Users\\priya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\encodings\\cp1252.py\", line 23, in decode\n",
      "    return codecs.charmap_decode(input,self.errors,decoding_table)[0]\n",
      "UnicodeDecodeError: 'charmap' codec can't decode byte 0x8d in position 187: character maps to <undefined>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Data: OK\n",
      "\n",
      "Checking file upload...\n",
      "File Upload: Status 202\n",
      "\n",
      "Checking endpoint responses...\n",
      "/api/v1/upload/list: OK\n",
      "/api/v1/models: OK\n"
     ]
    }
   ],
   "source": [
    "# Platform Health Check Summary\n",
    "import subprocess, requests, os\n",
    "\n",
    "print('Checking Python dependencies...')\n",
    "try:\n",
    "    result = subprocess.run(['pip', 'check'], capture_output=True, text=True)\n",
    "    print('Dependencies:', 'OK' if result.returncode == 0 else result.stdout)\n",
    "except Exception as e:\n",
    "    print('Dependency check error:', e)\n",
    "\n",
    "print('\\nRunning tests...')\n",
    "try:\n",
    "    result = subprocess.run(['python', 'test_platfrom.py'], capture_output=True, text=True)\n",
    "    print('Tests:', 'OK' if result.returncode == 0 else result.stdout)\n",
    "except Exception as e:\n",
    "    print('Test run error:', e)\n",
    "\n",
    "print('\\nChecking server status...')\n",
    "try:\n",
    "    resp = requests.get('http://127.0.0.1:8000')\n",
    "    print('Server:', 'OK' if resp.status_code == 200 else f'Status {resp.status_code}')\n",
    "except Exception as e:\n",
    "    print('Server error:', e)\n",
    "\n",
    "print('\\nChecking API docs...')\n",
    "try:\n",
    "    resp = requests.get('http://127.0.0.1:8000/docs')\n",
    "    print('API Docs:', 'OK' if resp.status_code == 200 else f'Status {resp.status_code}')\n",
    "except Exception as e:\n",
    "    print('Docs error:', e)\n",
    "\n",
    "print('\\nChecking sample data generation...')\n",
    "try:\n",
    "    result = subprocess.run(['python', 'generate_sample_data.py'], capture_output=True, text=True)\n",
    "    print('Sample Data:', 'OK' if result.returncode == 0 else result.stdout)\n",
    "except Exception as e:\n",
    "    print('Sample data error:', e)\n",
    "\n",
    "print('\\nChecking file upload...')\n",
    "try:\n",
    "    sample_file = 'data/samples/retail_sample.csv'\n",
    "    files = {'file': open(sample_file, 'rb')}\n",
    "    resp = requests.post('http://127.0.0.1:8000/api/v1/upload', files=files)\n",
    "    print('File Upload:', 'OK' if resp.status_code == 200 else f'Status {resp.status_code}')\n",
    "except Exception as e:\n",
    "    print('File upload error:', e)\n",
    "\n",
    "print('\\nChecking endpoint responses...')\n",
    "endpoints = [\n",
    "    '/api/v1/upload/list',\n",
    "    '/api/v1/models',\n",
    "]\n",
    "for ep in endpoints:\n",
    "    try:\n",
    "        resp = requests.get(f'http://127.0.0.1:8000{ep}')\n",
    "        print(f'{ep}:', 'OK' if resp.status_code == 200 else f'Status {resp.status_code}')\n",
    "    except Exception as e:\n",
    "        print(f'{ep} error:', e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retail_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
